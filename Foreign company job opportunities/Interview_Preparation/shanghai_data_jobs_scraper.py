#!/usr/bin/env python3
"""
ä¸Šæµ·å¤–ä¼æ•°æ®å²—ä½ä¿¡æ¯çˆ¬è™«
ä¸“æ³¨äºè·å–å¤–ä¼ï¼ˆè·¨å›½å…¬å¸ï¼‰åœ¨ä¸Šæµ·æ‹›è˜çš„æ•°æ®ç›¸å…³å²—ä½
"""

import json
import time
from datetime import datetime
from typing import List, Dict, Set
import re


def get_target_companies() -> List[Dict]:
    """
    è·å–ç›®æ ‡å¤–ä¼å…¬å¸åˆ—è¡¨
    é‡ç‚¹å…³æ³¨ç§‘æŠ€ã€é‡‘èã€å’¨è¯¢ç­‰æ•°æ®å¯†é›†å‹è¡Œä¸šçš„è·¨å›½å…¬å¸
    """
    return [
        # ç§‘æŠ€å·¨å¤´
        {"name": "Microsoft", "industry": "Technology", "cn_name": "å¾®è½¯"},
        {"name": "Google", "industry": "Technology", "cn_name": "è°·æ­Œ"},
        {"name": "Amazon", "industry": "Technology/E-commerce", "cn_name": "äºšé©¬é€Š"},
        {"name": "Apple", "industry": "Technology", "cn_name": "è‹¹æœ"},
        {"name": "Meta (Facebook)", "industry": "Technology", "cn_name": "Meta"},
        {"name": "IBM", "industry": "Technology/Consulting", "cn_name": "IBM"},
        {"name": "Oracle", "industry": "Technology/Database", "cn_name": "ç”²éª¨æ–‡"},
        {"name": "SAP", "industry": "Enterprise Software", "cn_name": "æ€çˆ±æ™®"},
        {"name": "Salesforce", "industry": "Cloud/CRM", "cn_name": "èµ›å¯Œæ—¶"},

        # ä¸“ä¸šæ•°æ®/AIå…¬å¸
        {"name": "Databricks", "industry": "Data Platform", "cn_name": "Databricks"},
        {"name": "Snowflake", "industry": "Data Cloud", "cn_name": "Snowflake"},
        {"name": "Tableau (Salesforce)", "industry": "BI/Analytics", "cn_name": "Tableau"},
        {"name": "Splunk", "industry": "Data Analytics", "cn_name": "Splunk"},

        # å’¨è¯¢å…¬å¸
        {"name": "McKinsey & Company", "industry": "Consulting", "cn_name": "éº¦è‚¯é”¡"},
        {"name": "BCG", "industry": "Consulting", "cn_name": "æ³¢å£«é¡¿å’¨è¯¢"},
        {"name": "Bain & Company", "industry": "Consulting", "cn_name": "è´æ©"},
        {"name": "Accenture", "industry": "Consulting/IT", "cn_name": "åŸƒæ£®å“²"},
        {"name": "Deloitte", "industry": "Consulting/Audit", "cn_name": "å¾·å‹¤"},
        {"name": "PwC", "industry": "Consulting/Audit", "cn_name": "æ™®åæ°¸é“"},
        {"name": "EY", "industry": "Consulting/Audit", "cn_name": "å®‰æ°¸"},
        {"name": "KPMG", "industry": "Consulting/Audit", "cn_name": "æ¯•é©¬å¨"},

        # é‡‘èæœºæ„
        {"name": "JPMorgan Chase", "industry": "Finance", "cn_name": "æ‘©æ ¹å¤§é€š"},
        {"name": "Goldman Sachs", "industry": "Finance", "cn_name": "é«˜ç››"},
        {"name": "Morgan Stanley", "industry": "Finance", "cn_name": "æ‘©æ ¹å£«ä¸¹åˆ©"},
        {"name": "Citibank", "industry": "Finance", "cn_name": "èŠ±æ——é“¶è¡Œ"},
        {"name": "HSBC", "industry": "Finance", "cn_name": "æ±‡ä¸°é“¶è¡Œ"},
        {"name": "Standard Chartered", "industry": "Finance", "cn_name": "æ¸£æ‰“é“¶è¡Œ"},

        # å¿«æ¶ˆ/é›¶å”®
        {"name": "Unilever", "industry": "FMCG", "cn_name": "è”åˆåˆ©å"},
        {"name": "P&G", "industry": "FMCG", "cn_name": "å®æ´"},
        {"name": "Coca-Cola", "industry": "Beverage", "cn_name": "å¯å£å¯ä¹"},
        {"name": "Nike", "industry": "Retail/Sports", "cn_name": "è€å…‹"},
        {"name": "Adidas", "industry": "Retail/Sports", "cn_name": "é˜¿è¿ªè¾¾æ–¯"},

        # æ±½è½¦
        {"name": "Tesla", "industry": "Automotive/Tech", "cn_name": "ç‰¹æ–¯æ‹‰"},
        {"name": "BMW", "industry": "Automotive", "cn_name": "å®é©¬"},
        {"name": "Mercedes-Benz", "industry": "Automotive", "cn_name": "å¥”é©°"},

        # åˆ¶è¯/åŒ»ç–—
        {"name": "Pfizer", "industry": "Pharmaceutical", "cn_name": "è¾‰ç‘"},
        {"name": "Roche", "industry": "Pharmaceutical", "cn_name": "ç½—æ°"},
        {"name": "Johnson & Johnson", "industry": "Healthcare", "cn_name": "å¼ºç”Ÿ"},
    ]


def get_data_job_positions() -> List[Dict]:
    """
    å®šä¹‰æ•°æ®ç›¸å…³å²—ä½ç±»å‹åŠå…¶å…³é”®è¯
    """
    return [
        {
            "title": "Data Engineer",
            "cn_title": "æ•°æ®å·¥ç¨‹å¸ˆ",
            "keywords": ["data engineer", "æ•°æ®å·¥ç¨‹å¸ˆ", "etl", "data pipeline"],
            "level": ["junior", "mid", "senior"]
        },
        {
            "title": "Data Analyst",
            "cn_title": "æ•°æ®åˆ†æå¸ˆ",
            "keywords": ["data analyst", "æ•°æ®åˆ†æå¸ˆ", "business analyst", "åˆ†æå¸ˆ"],
            "level": ["junior", "mid", "senior"]
        },
        {
            "title": "Data Scientist",
            "cn_title": "æ•°æ®ç§‘å­¦å®¶",
            "keywords": ["data scientist", "æ•°æ®ç§‘å­¦å®¶", "machine learning", "ml engineer"],
            "level": ["mid", "senior"]
        },
        {
            "title": "BI Developer/Analyst",
            "cn_title": "BIå¼€å‘/åˆ†æå¸ˆ",
            "keywords": ["bi developer", "bi analyst", "tableau", "power bi", "å•†ä¸šæ™ºèƒ½"],
            "level": ["junior", "mid", "senior"]
        },
        {
            "title": "Analytics Engineer",
            "cn_title": "åˆ†æå·¥ç¨‹å¸ˆ",
            "keywords": ["analytics engineer", "åˆ†æå·¥ç¨‹å¸ˆ", "dbt"],
            "level": ["mid", "senior"]
        },
        {
            "title": "Data Architect",
            "cn_title": "æ•°æ®æ¶æ„å¸ˆ",
            "keywords": ["data architect", "æ•°æ®æ¶æ„å¸ˆ", "solution architect"],
            "level": ["senior", "lead"]
        },
    ]


def simulate_job_listings() -> List[Dict]:
    """
    æ¨¡æ‹Ÿä»æ‹›è˜ç½‘ç«™æŠ“å–çš„æ•°æ®å²—ä½ä¿¡æ¯
    åŸºäºçœŸå®å¸‚åœºæƒ…å†µæ•´ç†
    """
    jobs = [
        {
            "company": "Microsoft",
            "cn_company": "å¾®è½¯",
            "position": "Senior Data Engineer",
            "location": "Shanghai, China",
            "salary_range": "40-70k RMB/month",
            "posted_date": "2026-02-10",
            "job_description": """
We are seeking a Senior Data Engineer to join our Azure Data Platform team in Shanghai.

Responsibilities:
- Design and build scalable data pipelines using Azure Data Factory, Databricks, and Synapse Analytics
- Develop and maintain data warehouse solutions for business analytics
- Collaborate with data scientists and analysts to ensure data quality and availability
- Implement data governance and security best practices
- Optimize ETL processes for performance and cost efficiency

Requirements:
- 5+ years of experience in data engineering
- Strong proficiency in SQL, Python, and Spark
- Experience with cloud data platforms (Azure, AWS, or GCP)
- Solid understanding of data warehousing concepts (Kimball, Data Vault)
- Experience with modern data stack tools (dbt, Airflow, etc.)
- Excellent communication skills in English (written and verbal)
- Bachelor's degree in Computer Science or related field

Preferred:
- Azure certifications (DP-203, DP-900)
- Experience with real-time streaming (Kafka, Event Hub)
- Knowledge of ML/AI concepts
- Prior experience in multinational companies
""",
            "skills": ["Azure", "Python", "SQL", "Spark", "Data Warehousing", "ETL", "English"],
            "source": "LinkedIn"
        },
        {
            "company": "Amazon",
            "cn_company": "äºšé©¬é€Š",
            "position": "Data Engineer II",
            "location": "Shanghai, China",
            "salary_range": "35-60k RMB/month",
            "posted_date": "2026-02-12",
            "job_description": """
Amazon Web Services (AWS) is looking for a Data Engineer to build next-generation data solutions.

Key Responsibilities:
- Build and maintain data pipelines using AWS services (Glue, EMR, Redshift, S3)
- Design dimensional data models for analytics and reporting
- Work with stakeholders to understand data requirements
- Implement data quality checks and monitoring
- Automate data workflows using Python and SQL

Basic Qualifications:
- 3+ years of data engineering experience
- Proficiency in SQL and at least one programming language (Python/Java)
- Experience with AWS or other cloud platforms
- Understanding of data warehouse architecture
- Strong problem-solving skills
- Good English communication skills

Preferred Qualifications:
- AWS certifications
- Experience with big data technologies (Hadoop, Spark)
- Knowledge of data visualization tools (QuickSight, Tableau)
- Experience in agile development environment
- Familiar with Git and CI/CD practices
""",
            "skills": ["AWS", "Python", "SQL", "Redshift", "Data Modeling", "ETL", "English"],
            "source": "Amazon Careers"
        },
        {
            "company": "McKinsey & Company",
            "cn_company": "éº¦è‚¯é”¡",
            "position": "Data Analyst",
            "location": "Shanghai, China",
            "salary_range": "30-50k RMB/month",
            "posted_date": "2026-02-08",
            "job_description": """
Join McKinsey's QuantumBlack team as a Data Analyst supporting advanced analytics projects.

What You'll Do:
- Conduct complex data analysis to support client consulting projects
- Develop dashboards and visualizations using Tableau/Power BI
- Perform statistical analysis and hypothesis testing
- Collaborate with consultants and data scientists
- Present findings to stakeholders and clients

What You'll Bring:
- 2-4 years of experience in data analysis
- Advanced SQL skills and proficiency in Python or R
- Experience with BI tools (Tableau, Power BI, Looker)
- Strong statistical and analytical thinking
- Excellent presentation and storytelling skills
- Fluent English (both written and spoken)
- Bachelor's degree in quantitative field

Bonus Points:
- Experience in consulting or professional services
- Knowledge of machine learning concepts
- Industry expertise (finance, retail, healthcare)
- Master's degree in relevant field
""",
            "skills": ["SQL", "Python", "Tableau", "Statistics", "Data Visualization", "English", "Presentation"],
            "source": "McKinsey Careers"
        },
        {
            "company": "Goldman Sachs",
            "cn_company": "é«˜ç››",
            "position": "Quantitative Data Analyst",
            "location": "Shanghai, China",
            "salary_range": "35-65k RMB/month",
            "posted_date": "2026-02-11",
            "job_description": """
Goldman Sachs is seeking a Quantitative Data Analyst for our Shanghai office.

Responsibilities:
- Analyze large financial datasets to identify trends and insights
- Build and maintain data models for risk and trading analytics
- Develop automated reporting solutions using Python and SQL
- Collaborate with traders, risk managers, and technology teams
- Ensure data accuracy and consistency across systems

Requirements:
- 3+ years of experience in financial data analysis
- Strong SQL and Python skills
- Experience with financial databases and market data
- Solid understanding of statistics and probability
- Excellent attention to detail
- Strong English communication skills
- Bachelor's degree in Finance, Mathematics, Computer Science, or related field

Preferred:
- Experience with time series analysis
- Knowledge of financial instruments and markets
- Familiarity with data visualization tools
- CFA or FRM certification
""",
            "skills": ["SQL", "Python", "Financial Analysis", "Statistics", "Data Modeling", "English"],
            "source": "Goldman Sachs Careers"
        },
        {
            "company": "Accenture",
            "cn_company": "åŸƒæ£®å“²",
            "position": "Data Warehouse Developer",
            "location": "Shanghai, China",
            "salary_range": "25-45k RMB/month",
            "posted_date": "2026-02-09",
            "job_description": """
Accenture is looking for Data Warehouse Developers to join our Data & Analytics practice.

Key Accountabilities:
- Design and develop data warehouse solutions (Kimball methodology)
- Build ETL processes using industry-standard tools
- Create dimensional models (star schema, snowflake schema)
- Optimize SQL queries and database performance
- Document technical specifications and data lineage
- Participate in requirement gathering and solution design

Must-Have Skills:
- 3-5 years of data warehouse development experience
- Expert-level SQL skills
- Experience with ETL tools (Informatica, SSIS, Talend, or similar)
- Understanding of dimensional modeling principles
- Database experience (Oracle, SQL Server, Teradata, or similar)
- Good English reading and writing skills

Nice-to-Have:
- Cloud data warehouse experience (Snowflake, Redshift, BigQuery)
- Scripting skills (Python, Shell)
- Experience with Agile/Scrum methodology
- Relevant certifications
""",
            "skills": ["SQL", "ETL", "Data Warehousing", "Dimensional Modeling", "Informatica", "English"],
            "source": "Accenture Careers"
        },
        {
            "company": "SAP",
            "cn_company": "æ€çˆ±æ™®",
            "position": "Analytics Engineer",
            "location": "Shanghai, China",
            "salary_range": "30-55k RMB/month",
            "posted_date": "2026-02-13",
            "job_description": """
SAP is hiring an Analytics Engineer to work on our cloud analytics platform.

What You'll Do:
- Transform raw data into analytics-ready datasets using dbt
- Design and implement metrics layer for business reporting
- Build and maintain data pipelines in cloud environments
- Work closely with analysts to understand data needs
- Establish data quality standards and testing frameworks
- Create documentation for data models and processes

What We're Looking For:
- 3+ years in analytics engineering or data engineering role
- Strong SQL skills and experience with modern data stack
- Hands-on experience with dbt (data build tool)
- Familiarity with cloud data platforms (Snowflake, BigQuery, Redshift)
- Understanding of software engineering best practices (Git, testing, CI/CD)
- Python knowledge is a plus
- Good English communication skills

Preferred:
- Experience with data orchestration tools (Airflow, Prefect, Dagster)
- Knowledge of BI tools and semantic layers
- Background in analytics or data science
""",
            "skills": ["SQL", "dbt", "Python", "Modern Data Stack", "Cloud Platforms", "Git", "English"],
            "source": "SAP Careers"
        },
        {
            "company": "Deloitte",
            "cn_company": "å¾·å‹¤",
            "position": "Senior Data Scientist",
            "location": "Shanghai, China",
            "salary_range": "40-80k RMB/month",
            "posted_date": "2026-02-07",
            "job_description": """
Deloitte Consulting is seeking a Senior Data Scientist for client-facing analytics projects.

Role Overview:
- Lead end-to-end data science projects for enterprise clients
- Develop predictive models and machine learning solutions
- Conduct advanced statistical analysis and experiments
- Translate business problems into analytical frameworks
- Present insights and recommendations to C-level executives
- Mentor junior team members

Required Skills:
- 5+ years of experience in data science or advanced analytics
- Strong foundation in statistics, machine learning, and algorithms
- Proficiency in Python (scikit-learn, pandas, numpy)
- Experience with SQL and data manipulation
- Proven track record of delivering business impact
- Excellent stakeholder management and presentation skills
- Fluent in English and Mandarin
- Master's or PhD in quantitative field preferred

Desirable:
- Consulting experience
- Experience with cloud ML platforms (AWS SageMaker, Azure ML, GCP Vertex AI)
- Knowledge of deep learning frameworks (TensorFlow, PyTorch)
- Industry expertise in financial services, retail, or manufacturing
""",
            "skills": ["Python", "Machine Learning", "Statistics", "SQL", "Consulting", "English", "Presentation"],
            "source": "Deloitte Careers"
        },
        {
            "company": "HSBC",
            "cn_company": "æ±‡ä¸°é“¶è¡Œ",
            "position": "Data Engineer - Risk Analytics",
            "location": "Shanghai, China",
            "salary_range": "35-60k RMB/month",
            "posted_date": "2026-02-10",
            "job_description": """
HSBC is looking for a Data Engineer to support risk analytics and regulatory reporting.

Responsibilities:
- Build data pipelines for credit risk, market risk, and operational risk
- Develop ETL processes to consolidate data from multiple sources
- Work with risk analysts and compliance teams
- Ensure data quality and regulatory compliance (Basel III, IFRS9)
- Optimize database performance for large-scale risk calculations
- Support regulatory reporting and stress testing exercises

Requirements:
- 4+ years of data engineering experience, preferably in banking/finance
- Strong SQL skills and database knowledge (Oracle, SQL Server, Teradata)
- Experience with ETL tools and data integration
- Understanding of banking products and risk concepts
- Attention to detail and commitment to data accuracy
- Good English communication skills
- Bachelor's degree in Computer Science, Engineering, or related field

Preferred:
- Experience with regulatory reporting (FRTB, IFRS9, etc.)
- Knowledge of big data technologies (Hadoop, Spark)
- Python or R programming skills
- Relevant certifications (FRM, PRM)
""",
            "skills": ["SQL", "ETL", "Risk Analytics", "Banking", "Oracle", "Data Quality", "English"],
            "source": "HSBC Careers"
        },
        {
            "company": "Apple",
            "cn_company": "è‹¹æœ",
            "position": "Data Engineer - Supply Chain Analytics",
            "location": "Shanghai, China",
            "salary_range": "40-70k RMB/month",
            "posted_date": "2026-02-12",
            "job_description": """
Apple is seeking a Data Engineer to support supply chain and operations analytics.

Key Responsibilities:
- Design and implement data pipelines for supply chain data
- Build data models to support inventory, logistics, and procurement analytics
- Collaborate with operations teams across Asia-Pacific region
- Develop automated reporting and monitoring solutions
- Ensure data integrity and consistency across systems
- Optimize data infrastructure for scale and performance

Minimum Qualifications:
- 5+ years of experience in data engineering
- Expert SQL and Python programming skills
- Experience with distributed computing (Spark, Hadoop)
- Strong understanding of data warehousing and ETL concepts
- Ability to work in fast-paced, dynamic environment
- Excellent problem-solving and analytical skills
- Proficient in English

Preferred Qualifications:
- Experience in supply chain or operations analytics
- Knowledge of real-time data processing (Kafka, Flink)
- Familiarity with data orchestration tools (Airflow)
- Cloud platform experience (AWS, GCP, or Azure)
- Background in manufacturing or retail industry
""",
            "skills": ["Python", "SQL", "Spark", "Supply Chain", "ETL", "Cloud", "English"],
            "source": "Apple Careers"
        },
        {
            "company": "Unilever",
            "cn_company": "è”åˆåˆ©å",
            "position": "Business Intelligence Analyst",
            "location": "Shanghai, China",
            "salary_range": "25-45k RMB/month",
            "posted_date": "2026-02-11",
            "job_description": """
Unilever is hiring a BI Analyst to support commercial analytics for FMCG brands.

What You'll Do:
- Create dashboards and reports to track business KPIs
- Analyze sales, marketing, and consumer data
- Support brand teams with ad-hoc analysis
- Maintain and enhance BI infrastructure (Tableau, Power BI)
- Collaborate with regional and global analytics teams
- Identify opportunities for process automation

What You Need:
- 2-4 years of experience in BI or data analysis
- Strong SQL and Excel skills
- Hands-on experience with BI tools (Tableau, Power BI, or Looker)
- Understanding of FMCG/retail business metrics
- Ability to tell stories with data
- Good English skills for global collaboration
- Bachelor's degree in Business, Statistics, or related field

Bonus:
- Python or R programming experience
- Knowledge of marketing analytics
- Experience with Google Analytics or similar tools
- Understanding of consumer behavior
""",
            "skills": ["SQL", "Tableau", "Power BI", "Excel", "FMCG", "Marketing Analytics", "English"],
            "source": "Unilever Careers"
        },
    ]

    return jobs


def analyze_skill_requirements(jobs: List[Dict]) -> Dict:
    """
    åˆ†æå²—ä½æŠ€èƒ½è¦æ±‚ï¼Œæå–å…³é”®ä¿¡æ¯
    """

    # ç»Ÿè®¡æŠ€èƒ½å‡ºç°é¢‘ç‡
    skill_count = {}
    for job in jobs:
        for skill in job.get("skills", []):
            skill_count[skill] = skill_count.get(skill, 0) + 1

    # æŒ‰é¢‘ç‡æ’åº
    sorted_skills = sorted(skill_count.items(), key=lambda x: x[1], reverse=True)

    # æå–è–ªèµ„ä¿¡æ¯
    salary_ranges = []
    for job in jobs:
        salary = job.get("salary_range", "")
        # æå–æ•°å­—èŒƒå›´
        matches = re.findall(r'(\d+)-(\d+)k', salary)
        if matches:
            low, high = matches[0]
            salary_ranges.append((int(low), int(high)))

    avg_low = sum(s[0] for s in salary_ranges) / len(salary_ranges) if salary_ranges else 0
    avg_high = sum(s[1] for s in salary_ranges) / len(salary_ranges) if salary_ranges else 0

    # åˆ†ç±»æŠ€èƒ½
    technical_skills = {}
    soft_skills = {}
    tools_platforms = {}

    for skill, count in sorted_skills:
        skill_lower = skill.lower()

        # æŠ€æœ¯æŠ€èƒ½
        if any(tech in skill_lower for tech in ['sql', 'python', 'spark', 'etl', 'java', 'scala', 'r']):
            technical_skills[skill] = count
        # å·¥å…·å’Œå¹³å°
        elif any(tool in skill_lower for tool in ['aws', 'azure', 'gcp', 'tableau', 'power bi', 'snowflake', 'airflow', 'dbt', 'hadoop', 'kafka', 'oracle']):
            tools_platforms[skill] = count
        # è½¯æŠ€èƒ½å’Œé¢†åŸŸçŸ¥è¯†
        elif any(soft in skill_lower for soft in ['english', 'presentation', 'consulting', 'communication']):
            soft_skills[skill] = count
        else:
            # å…¶ä»–å½’ç±»ä¸ºæŠ€æœ¯æŠ€èƒ½
            technical_skills[skill] = count

    return {
        "total_jobs": len(jobs),
        "skill_frequency": dict(sorted_skills[:20]),  # Top 20
        "technical_skills": technical_skills,
        "tools_platforms": tools_platforms,
        "soft_skills": soft_skills,
        "salary_analysis": {
            "average_low": f"{avg_low:.1f}k RMB/month",
            "average_high": f"{avg_high:.1f}k RMB/month",
            "range": f"{min(s[0] for s in salary_ranges)}-{max(s[1] for s in salary_ranges)}k RMB/month" if salary_ranges else "N/A"
        }
    }


def identify_skill_gaps(current_skills: Set[str], target_skills: Dict) -> Dict:
    """
    è¯†åˆ«æŠ€èƒ½å·®è·

    Args:
        current_skills: å½“å‰å…·å¤‡çš„æŠ€èƒ½é›†åˆ
        target_skills: ç›®æ ‡å²—ä½è¦æ±‚çš„æŠ€èƒ½å­—å…¸ï¼ˆæŠ€èƒ½: å‡ºç°é¢‘ç‡ï¼‰

    Returns:
        æŠ€èƒ½å·®è·åˆ†æç»“æœ
    """

    # è½¬æ¢ä¸ºå°å†™ä¾¿äºæ¯”è¾ƒ
    current_skills_lower = {s.lower() for s in current_skills}

    # è¯†åˆ«å·²æœ‰æŠ€èƒ½å’Œç¼ºå¤±æŠ€èƒ½
    have_skills = []
    missing_skills = []

    for skill, freq in target_skills.items():
        skill_lower = skill.lower()
        if any(cs in skill_lower or skill_lower in cs for cs in current_skills_lower):
            have_skills.append((skill, freq))
        else:
            missing_skills.append((skill, freq))

    # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆå‡ºç°é¢‘ç‡ï¼‰
    missing_skills.sort(key=lambda x: x[1], reverse=True)

    return {
        "have_skills": have_skills,
        "missing_skills": missing_skills,
        "skill_coverage": len(have_skills) / len(target_skills) * 100 if target_skills else 0
    }


def generate_learning_plan(skill_gaps: Dict, timeline_months: int = 6) -> Dict:
    """
    ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’

    Args:
        skill_gaps: æŠ€èƒ½å·®è·åˆ†æç»“æœ
        timeline_months: å­¦ä¹ æ—¶é—´çº¿ï¼ˆæœˆï¼‰

    Returns:
        è¯¦ç»†çš„å­¦ä¹ è®¡åˆ’
    """

    missing_skills = skill_gaps["missing_skills"]

    # æŠ€èƒ½å­¦ä¹ èµ„æºæ˜ å°„
    skill_resources = {
        # ç¼–ç¨‹è¯­è¨€
        "python": {
            "priority": "Critical",
            "learning_time": "1-2 months",
            "resources": [
                "Pythonå®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹",
                "ã€ŠPython Crash Courseã€‹ä¹¦ç±",
                "LeetCode Pythonä¸“é¢˜ç»ƒä¹ ",
                "DataCamp Python for Data Engineeringè¯¾ç¨‹"
            ],
            "practice_projects": [
                "ç¼–å†™æ•°æ®æ¸…æ´—è„šæœ¬å¤„ç†CSV/JSONæ–‡ä»¶",
                "ä½¿ç”¨pandasè¿›è¡Œæ•°æ®åˆ†æ",
                "å¼€å‘ç®€å•çš„ETLè„šæœ¬",
                "å®ç°å¸¸è§ç®—æ³•å’Œæ•°æ®ç»“æ„"
            ]
        },
        "sql": {
            "priority": "Critical",
            "learning_time": "2-3 weeks",
            "resources": [
                "Mode Analytics SQLæ•™ç¨‹",
                "LeetCode Databaseä¸“é¢˜ï¼ˆ180+é¢˜ï¼‰",
                "ã€ŠSQL Performance Explainedã€‹",
                "HackerRank SQLç»ƒä¹ "
            ],
            "practice_projects": [
                "è§£å†³50+ SQLå¤æ‚æŸ¥è¯¢é¢˜ç›®",
                "åˆ†æçª—å£å‡½æ•°å’ŒCTEåº”ç”¨åœºæ™¯",
                "å­¦ä¹ æŸ¥è¯¢ä¼˜åŒ–å’Œç´¢å¼•ç­–ç•¥",
                "å®è·µæ•°æ®åº“è®¾è®¡èŒƒå¼"
            ]
        },

        # äº‘å¹³å°
        "aws": {
            "priority": "High",
            "learning_time": "1-2 months",
            "resources": [
                "AWSå®˜æ–¹åŸ¹è®­è¯¾ç¨‹ï¼ˆå…è´¹ï¼‰",
                "A Cloud Guru AWSè¯¾ç¨‹",
                "AWS Solutions Architect Associateè®¤è¯å¤‡è€ƒ",
                "AWSæ•°æ®å·¥ç¨‹æœåŠ¡å®è·µï¼ˆGlue, EMR, Redshiftï¼‰"
            ],
            "practice_projects": [
                "åœ¨AWSå…è´¹å¥—é¤æ­å»ºæ•°æ®ç®¡é“",
                "ä½¿ç”¨S3 + Glue + Athenaæ„å»ºæ•°æ®æ¹–",
                "é…ç½®Redshiftæ•°æ®ä»“åº“",
                "å®ç°Lambda + EventBridgeè‡ªåŠ¨åŒ–ä»»åŠ¡"
            ]
        },
        "azure": {
            "priority": "High",
            "learning_time": "1-2 months",
            "resources": [
                "Microsoft Learn Azureæ•°æ®å·¥ç¨‹è·¯å¾„",
                "Azure Data Engineer Associate (DP-203)è®¤è¯",
                "Pluralsight Azureè¯¾ç¨‹",
                "Azureæ•°æ®æœåŠ¡å®è·µï¼ˆData Factory, Synapse, Databricksï¼‰"
            ],
            "practice_projects": [
                "ä½¿ç”¨Azure Data Factoryåˆ›å»ºETLç®¡é“",
                "åœ¨Azure Databricksè¿è¡ŒSparkä½œä¸š",
                "é…ç½®Azure Synapse Analytics",
                "å®ç°Azure DevOps CI/CD"
            ]
        },
        "gcp": {
            "priority": "Medium",
            "learning_time": "1-2 months",
            "resources": [
                "Google Cloud Skills Boost",
                "Coursera GCPä¸“é¡¹è¯¾ç¨‹",
                "ã€ŠData Engineering on Google Cloud Platformã€‹",
                "GCP Professional Data Engineerè®¤è¯"
            ],
            "practice_projects": [
                "ä½¿ç”¨BigQueryè¿›è¡Œæ•°æ®åˆ†æ",
                "æ„å»ºCloud Composer (Airflow)å·¥ä½œæµ",
                "å®ç°Dataflowæµå¼å¤„ç†",
                "é…ç½®Cloud Storageæ•°æ®æ¹–"
            ]
        },

        # å¤§æ•°æ®æŠ€æœ¯
        "spark": {
            "priority": "High",
            "learning_time": "2-3 months",
            "resources": [
                "ã€ŠLearning Sparkã€‹ç¬¬äºŒç‰ˆ",
                "Databricks SparkåŸ¹è®­",
                "Udemy Sparkè¯¾ç¨‹",
                "Apache Sparkå®˜æ–¹æ–‡æ¡£"
            ],
            "practice_projects": [
                "ç”¨PySparkå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†",
                "å®ç°Spark SQLæ•°æ®è½¬æ¢",
                "ä¼˜åŒ–Sparkä½œä¸šæ€§èƒ½",
                "å­¦ä¹ Spark Streamingå®æ—¶å¤„ç†"
            ]
        },

        # æ•°æ®ä»“åº“
        "data warehousing": {
            "priority": "High",
            "learning_time": "1-2 months",
            "resources": [
                "ã€ŠThe Data Warehouse Toolkitã€‹(Kimball)",
                "ã€ŠBuilding the Data Warehouseã€‹(Inmon)",
                "Courseraæ•°æ®ä»“åº“ä¸“é¡¹è¯¾ç¨‹",
                "Modern Data Warehouseæ¶æ„æ–‡ç« "
            ],
            "practice_projects": [
                "è®¾è®¡Kimballç»´åº¦æ¨¡å‹ï¼ˆæ˜Ÿå‹/é›ªèŠ±ï¼‰",
                "å®ç°SCDï¼ˆç¼“æ…¢å˜åŒ–ç»´ï¼‰",
                "æ„å»ºäº‹å®è¡¨å’Œç»´åº¦è¡¨",
                "å­¦ä¹ Data Vault 2.0å»ºæ¨¡"
            ]
        },

        # ç°ä»£æ•°æ®æ ˆ
        "dbt": {
            "priority": "Medium-High",
            "learning_time": "2-4 weeks",
            "resources": [
                "dbtå®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹",
                "dbt Learnå…è´¹è¯¾ç¨‹",
                "ã€ŠAnalytics Engineering with dbtã€‹",
                "dbt Discourseç¤¾åŒº"
            ],
            "practice_projects": [
                "æ­å»ºdbté¡¹ç›®ç»“æ„",
                "ç¼–å†™dbtæ¨¡å‹å’Œæµ‹è¯•",
                "å®ç°å¢é‡æ¨¡å‹å’Œå¿«ç…§",
                "é…ç½®dbt Cloud CI/CD"
            ]
        },
        "airflow": {
            "priority": "Medium",
            "learning_time": "3-4 weeks",
            "resources": [
                "Apache Airflowå®˜æ–¹æ–‡æ¡£",
                "ã€ŠData Pipelines with Apache Airflowã€‹",
                "Astronomer Airflowæ•™ç¨‹",
                "Airflow Summitè§†é¢‘"
            ],
            "practice_projects": [
                "åˆ›å»ºAirflow DAGè°ƒåº¦ä»»åŠ¡",
                "å®ç°ä»»åŠ¡ä¾èµ–å’Œé”™è¯¯å¤„ç†",
                "é…ç½®Airflowè¿æ¥å’Œå˜é‡",
                "å­¦ä¹ TaskFlow API"
            ]
        },

        # BIå·¥å…·
        "tableau": {
            "priority": "Medium",
            "learning_time": "2-3 weeks",
            "resources": [
                "Tableau Desktop Specialistè®¤è¯",
                "Tableau Public Galleryå­¦ä¹ ",
                "ã€ŠTableau Your Dataã€‹ä¹¦ç±",
                "Tableauå®˜æ–¹åŸ¹è®­è§†é¢‘"
            ],
            "practice_projects": [
                "åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿",
                "å®ç°é«˜çº§è®¡ç®—å’ŒLODè¡¨è¾¾å¼",
                "è¿æ¥å¤šæ•°æ®æºè¿›è¡Œæ··åˆ",
                "å‘å¸ƒåˆ°Tableau Server/Online"
            ]
        },
        "power bi": {
            "priority": "Medium",
            "learning_time": "2-3 weeks",
            "resources": [
                "Microsoft Learn Power BIè·¯å¾„",
                "ã€ŠDashboarding and Reporting with Power BIã€‹",
                "SQLBIç½‘ç«™DAXæ•™ç¨‹",
                "Power BI Communityè®ºå›"
            ],
            "practice_projects": [
                "åˆ›å»ºPower BIæŠ¥è¡¨å’Œä»ªè¡¨æ¿",
                "å­¦ä¹ DAXè¯­è¨€å’Œæ•°æ®å»ºæ¨¡",
                "å®ç°RLSï¼ˆè¡Œçº§å®‰å…¨ï¼‰",
                "é…ç½®Power BI Serviceå‘å¸ƒ"
            ]
        },

        # è‹±è¯­
        "english": {
            "priority": "Critical",
            "learning_time": "Ongoing (6 months)",
            "resources": [
                "èŒåœºè‹±è¯­å£è¯­è¯¾ç¨‹ï¼ˆå¦‚Wall Street Englishï¼‰",
                "æŠ€æœ¯è‹±è¯­é˜…è¯»ï¼ˆMedium, Dev.toæ–‡ç« ï¼‰",
                "å‚åŠ è‹±è¯­è§’æˆ–è¯­è¨€äº¤æ¢",
                "çœ‹è‹±æ–‡æŠ€æœ¯è§†é¢‘ï¼ˆYouTube, Pluralsightï¼‰"
            ],
            "practice_projects": [
                "æ¯å¤©é˜…è¯»è‹±æ–‡æŠ€æœ¯åšå®¢",
                "ç”¨è‹±æ–‡å†™æŠ€æœ¯æ–‡æ¡£",
                "å‚åŠ è‹±æ–‡æŠ€æœ¯åˆ†äº«ä¼š",
                "æ¨¡æ‹Ÿè‹±æ–‡é¢è¯•ç»ƒä¹ "
            ]
        },

        # å…¶ä»–é‡è¦æŠ€èƒ½
        "git": {
            "priority": "High",
            "learning_time": "1-2 weeks",
            "resources": [
                "ã€ŠPro Gitã€‹å…è´¹ç”µå­ä¹¦",
                "GitHub Learning Lab",
                "Learn Git Branchingäº’åŠ¨æ•™ç¨‹",
                "Gitå®˜æ–¹æ–‡æ¡£"
            ],
            "practice_projects": [
                "æŒæ¡GitåŸºæœ¬å‘½ä»¤å’Œå·¥ä½œæµ",
                "å­¦ä¹ åˆ†æ”¯ç®¡ç†å’Œåˆå¹¶ç­–ç•¥",
                "å®è·µPull Requestæµç¨‹",
                "äº†è§£Git Hookså’ŒCI/CDé›†æˆ"
            ]
        },
        "statistics": {
            "priority": "Medium",
            "learning_time": "1-2 months",
            "resources": [
                "ã€ŠStatistics for Business and Economicsã€‹",
                "Khan Academyç»Ÿè®¡å­¦è¯¾ç¨‹",
                "Courseraç»Ÿè®¡æ¨æ–­ä¸“é¡¹è¯¾ç¨‹",
                "ã€ŠPractical Statistics for Data Scientistsã€‹"
            ],
            "practice_projects": [
                "æŒæ¡æè¿°æ€§ç»Ÿè®¡å’Œæ¨æ–­ç»Ÿè®¡",
                "å­¦ä¹ å‡è®¾æ£€éªŒå’Œç½®ä¿¡åŒºé—´",
                "ç†è§£A/Bæµ‹è¯•åŸç†",
                "å®è·µå›å½’åˆ†æå’Œç›¸å…³åˆ†æ"
            ]
        }
    }

    # æ„å»ºå­¦ä¹ è®¡åˆ’
    learning_plan = {
        "overview": {
            "timeline": f"{timeline_months} months",
            "goal": "ä»å›½å†…äº’è”ç½‘ä¼ ç»Ÿæ•°ä»“å²—ä½è½¬å‹åˆ°å¤–ä¼æ•°æ®å²—ä½",
            "focus_areas": ["æŠ€æœ¯èƒ½åŠ›æå‡", "è‹±è¯­æ²Ÿé€šèƒ½åŠ›", "å¤–ä¼å·¥ä½œæ–‡åŒ–é€‚åº”"]
        },
        "monthly_plan": {},
        "skill_roadmap": [],
        "certifications": [],
        "english_improvement": {},
        "job_preparation": {}
    }

    # æŒ‰ä¼˜å…ˆçº§å’Œé¢‘ç‡ç¡®å®šå­¦ä¹ é¡ºåº
    priority_map = {"Critical": 1, "High": 2, "Medium-High": 2.5, "Medium": 3, "Low": 4}

    skills_to_learn = []
    for skill, freq in missing_skills:
        skill_key = skill.lower()
        for key in skill_resources:
            if key in skill_key or skill_key in key:
                resource = skill_resources[key]
                priority_score = priority_map.get(resource["priority"], 3)
                # ç»¼åˆè€ƒè™‘ä¼˜å…ˆçº§å’Œå‡ºç°é¢‘ç‡
                score = priority_score - (freq / 10)  # é¢‘ç‡è¶Šé«˜ï¼Œåˆ†æ•°è¶Šä½ï¼ˆä¼˜å…ˆçº§è¶Šé«˜ï¼‰
                skills_to_learn.append({
                    "skill": skill,
                    "details": resource,
                    "frequency": freq,
                    "score": score
                })
                break

    # æ’åº
    skills_to_learn.sort(key=lambda x: x["score"])

    # åˆ†é…åˆ°å„æœˆ
    if timeline_months == 3:
        # 3ä¸ªæœˆå¿«é€Ÿé€šé“ï¼ˆé«˜å¼ºåº¦ï¼‰
        learning_plan["monthly_plan"] = {
            "Month 1": {
                "focus": "æ ¸å¿ƒæŠ€æœ¯åŸºç¡€ + è‹±è¯­å¯åŠ¨",
                "skills": [],
                "weekly_hours": "20-25å°æ—¶",
                "milestones": []
            },
            "Month 2": {
                "focus": "äº‘å¹³å° + ç°ä»£æ•°æ®æ ˆ + è‹±è¯­å¼ºåŒ–",
                "skills": [],
                "weekly_hours": "20-25å°æ—¶",
                "milestones": []
            },
            "Month 3": {
                "focus": "ç»¼åˆé¡¹ç›® + é¢è¯•å‡†å¤‡",
                "skills": [],
                "weekly_hours": "15-20å°æ—¶",
                "milestones": []
            }
        }

        # åˆ†é…æŠ€èƒ½åˆ°å„æœˆ
        for i, skill_info in enumerate(skills_to_learn[:8]):  # 3ä¸ªæœˆæœ€å¤š8-10ä¸ªé‡ç‚¹æŠ€èƒ½
            if i < 3:
                month = "Month 1"
            elif i < 6:
                month = "Month 2"
            else:
                month = "Month 3"
            learning_plan["monthly_plan"][month]["skills"].append(skill_info)

    elif timeline_months == 6:
        # 6ä¸ªæœˆæ ‡å‡†é€šé“ï¼ˆä¸­ç­‰å¼ºåº¦ï¼‰
        learning_plan["monthly_plan"] = {
            "Month 1-2": {
                "focus": "ç¼–ç¨‹åŸºç¡€ + SQLç²¾è¿›",
                "skills": [],
                "weekly_hours": "15-20å°æ—¶",
                "milestones": ["å®ŒæˆPythonæ ¸å¿ƒè¯­æ³•", "è§£å†³100+ SQLé¢˜ç›®", "è‹±è¯­æŠ€æœ¯é˜…è¯»å¯åŠ¨"]
            },
            "Month 3-4": {
                "focus": "äº‘å¹³å° + å¤§æ•°æ®æŠ€æœ¯",
                "skills": [],
                "weekly_hours": "15-20å°æ—¶",
                "milestones": ["è·å¾—äº‘å¹³å°è®¤è¯", "å®ŒæˆSparké¡¹ç›®", "è‹±è¯­å£è¯­æå‡"]
            },
            "Month 5": {
                "focus": "ç°ä»£æ•°æ®æ ˆ + BIå·¥å…·",
                "skills": [],
                "weekly_hours": "12-15å°æ—¶",
                "milestones": ["æŒæ¡dbtå’ŒAirflow", "åˆ›å»ºBIä»ªè¡¨æ¿ä½œå“é›†"]
            },
            "Month 6": {
                "focus": "ç»¼åˆé¡¹ç›® + é¢è¯•å‡†å¤‡",
                "skills": [],
                "weekly_hours": "10-15å°æ—¶",
                "milestones": ["å®Œæˆç«¯åˆ°ç«¯æ•°æ®é¡¹ç›®", "å‡†å¤‡è‹±æ–‡ç®€å†å’Œé¢è¯•", "å¼€å§‹æŠ•é€’ç®€å†"]
            }
        }

        # åˆ†é…æŠ€èƒ½
        skills_per_phase = [3, 3, 2, 2]
        phases = ["Month 1-2", "Month 3-4", "Month 5", "Month 6"]
        idx = 0
        for phase, count in zip(phases, skills_per_phase):
            for _ in range(count):
                if idx < len(skills_to_learn):
                    learning_plan["monthly_plan"][phase]["skills"].append(skills_to_learn[idx])
                    idx += 1

    # æŠ€èƒ½è·¯çº¿å›¾
    learning_plan["skill_roadmap"] = [
        {
            "phase": "Foundation (ç¬¬1-2æœˆ)",
            "goals": [
                "å¼ºåŒ–Pythonç¼–ç¨‹ï¼ˆæ•°æ®å¤„ç†ã€è„šæœ¬å¼€å‘ï¼‰",
                "ç²¾é€šSQLï¼ˆå¤æ‚æŸ¥è¯¢ã€æ€§èƒ½ä¼˜åŒ–ã€çª—å£å‡½æ•°ï¼‰",
                "æŒæ¡Gitç‰ˆæœ¬æ§åˆ¶",
                "å¼€å§‹æŠ€æœ¯è‹±è¯­å­¦ä¹ "
            ]
        },
        {
            "phase": "Cloud & Big Data (ç¬¬3-4æœˆ)",
            "goals": [
                "å­¦ä¹ è‡³å°‘ä¸€ä¸ªäº‘å¹³å°ï¼ˆAWS/Azureï¼Œæ ¹æ®ç›®æ ‡å…¬å¸é€‰æ‹©ï¼‰",
                "æŒæ¡Sparkå¤§æ•°æ®å¤„ç†",
                "ç†è§£äº‘æ•°æ®ä»“åº“æ¶æ„",
                "æå‡è‹±è¯­é˜…è¯»å’Œå†™ä½œ"
            ]
        },
        {
            "phase": "Modern Data Stack (ç¬¬5æœˆ)",
            "goals": [
                "å­¦ä¹ dbtè¿›è¡Œæ•°æ®è½¬æ¢",
                "æŒæ¡Airflowä»»åŠ¡è°ƒåº¦",
                "ç†Ÿç»ƒä½¿ç”¨BIå·¥å…·ï¼ˆTableauæˆ–Power BIï¼‰",
                "ç»ƒä¹ è‹±è¯­å£è¯­è¡¨è¾¾"
            ]
        },
        {
            "phase": "Integration & Job Hunt (ç¬¬6æœˆ)",
            "goals": [
                "å®Œæˆç»¼åˆæ•°æ®å·¥ç¨‹é¡¹ç›®",
                "å‡†å¤‡è‹±æ–‡ç®€å†å’Œä½œå“é›†",
                "æ¨¡æ‹Ÿè‹±æ–‡æŠ€æœ¯é¢è¯•",
                "å¼€å§‹æŠ•é€’å¤–ä¼èŒä½"
            ]
        }
    ]

    # æ¨èè®¤è¯
    learning_plan["certifications"] = [
        {
            "name": "AWS Certified Data Analytics - Specialty",
            "provider": "Amazon Web Services",
            "difficulty": "Medium",
            "prep_time": "1-2 months",
            "value": "Very High for AWS-focused roles",
            "cost": "$300"
        },
        {
            "name": "Microsoft Certified: Azure Data Engineer Associate (DP-203)",
            "provider": "Microsoft",
            "difficulty": "Medium",
            "prep_time": "1-2 months",
            "value": "Very High for Azure-focused roles",
            "cost": "$165"
        },
        {
            "name": "Google Cloud Professional Data Engineer",
            "provider": "Google Cloud",
            "difficulty": "Hard",
            "prep_time": "2-3 months",
            "value": "High for GCP-focused roles",
            "cost": "$200"
        },
        {
            "name": "dbt Analytics Engineering Certification",
            "provider": "dbt Labs",
            "difficulty": "Easy-Medium",
            "prep_time": "2-3 weeks",
            "value": "High for modern data stack roles",
            "cost": "Free"
        },
        {
            "name": "Tableau Desktop Specialist",
            "provider": "Tableau",
            "difficulty": "Easy",
            "prep_time": "2-3 weeks",
            "value": "Medium for BI-focused roles",
            "cost": "$100"
        }
    ]

    # è‹±è¯­æå‡è®¡åˆ’
    learning_plan["english_improvement"] = {
        "daily_routine": [
            "æ™¨è¯»ï¼š20åˆ†é’Ÿè‹±æ–‡æŠ€æœ¯æ–‡ç« æˆ–æ–‡æ¡£",
            "åˆä¼‘ï¼šæ”¶å¬è‹±æ–‡æŠ€æœ¯æ’­å®¢ï¼ˆData Engineering Podcast, Software Engineering Dailyï¼‰",
            "æ™šä¸Šï¼šçœ‹è‹±æ–‡æŠ€æœ¯è§†é¢‘ï¼ˆYouTube, Pluralsightï¼‰30åˆ†é’Ÿ"
        ],
        "weekly_practice": [
            "å‚åŠ 1-2æ¬¡è‹±è¯­è§’æˆ–çº¿ä¸Šè¯­è¨€äº¤æ¢",
            "å†™1ç¯‡è‹±æ–‡æŠ€æœ¯åšå®¢æˆ–æ€»ç»“",
            "æ¨¡æ‹Ÿ1æ¬¡è‹±æ–‡æŠ€æœ¯é¢è¯•"
        ],
        "resources": [
            "æŠ€æœ¯è‹±è¯­è¯æ±‡è¡¨ï¼ˆæ•°æ®å·¥ç¨‹ç›¸å…³æœ¯è¯­ï¼‰",
            "STARé¢è¯•æ³•è‹±æ–‡å›ç­”æ¨¡æ¿",
            "å¸¸è§æŠ€æœ¯é¢è¯•é—®é¢˜è‹±æ–‡ç‰ˆ",
            "LinkedIn LearningèŒåœºè‹±è¯­è¯¾ç¨‹"
        ],
        "milestone_goals": {
            "Month 2": "èƒ½æµç•…é˜…è¯»è‹±æ–‡æŠ€æœ¯æ–‡æ¡£",
            "Month 4": "èƒ½ç”¨è‹±æ–‡å†™æ¸…æ™°çš„æŠ€æœ¯é‚®ä»¶å’Œæ–‡æ¡£",
            "Month 6": "èƒ½ç”¨è‹±æ–‡è¿›è¡ŒæŠ€æœ¯é¢è¯•å’Œæ—¥å¸¸å·¥ä½œæ²Ÿé€š"
        }
    }

    # æ±‚èŒå‡†å¤‡
    learning_plan["job_preparation"] = {
        "resume": [
            "ä½¿ç”¨è‹±æ–‡ç®€å†æ¨¡æ¿ï¼ˆé’ˆå¯¹å¤–ä¼ï¼‰",
            "çªå‡ºé‡åŒ–æˆæœï¼ˆprocessed XX TB data, improved performance by XX%ï¼‰",
            "å¼ºè°ƒäº‘å¹³å°å’Œç°ä»£å·¥å…·ç»éªŒ",
            "æ·»åŠ GitHubé¡¹ç›®é“¾æ¥",
            "è¯·æ¯è¯­äººå£«æˆ–ä¸“ä¸šæœåŠ¡æ¶¦è‰²"
        ],
        "portfolio": [
            "åœ¨GitHubæ­å»ºæ•°æ®å·¥ç¨‹é¡¹ç›®å±•ç¤º",
            "é¡¹ç›®1ï¼šäº‘å¹³å°æ•°æ®ç®¡é“ï¼ˆAWS/Azure + Airflow + dbtï¼‰",
            "é¡¹ç›®2ï¼šå®æ—¶æ•°æ®å¤„ç†ï¼ˆKafka + Spark Streamingï¼‰",
            "é¡¹ç›®3ï¼šBIä»ªè¡¨æ¿ï¼ˆTableau/Power BI with storytellingï¼‰",
            "æ‰€æœ‰é¡¹ç›®åŒ…å«è¯¦ç»†è‹±æ–‡READMEå’Œæ–‡æ¡£"
        ],
        "interview_prep": [
            "SQLåˆ·é¢˜ï¼šLeetCode Databaseæ‰€æœ‰Medium/Hardé¢˜ç›®",
            "Pythonç¼–ç¨‹ï¼šæŒæ¡æ•°æ®ç»“æ„å’Œç®—æ³•åŸºç¡€",
            "ç³»ç»Ÿè®¾è®¡ï¼šå­¦ä¹ æ•°æ®ç³»ç»Ÿè®¾è®¡ï¼ˆå‚è€ƒã€ŠDesigning Data-Intensive Applicationsã€‹ï¼‰",
            "è¡Œä¸ºé¢è¯•ï¼šå‡†å¤‡STARæ ¼å¼è‹±æ–‡å›ç­”",
            "æ¨¡æ‹Ÿé¢è¯•ï¼šé€šè¿‡Prampæˆ–æœ‹å‹è¿›è¡Œè‹±æ–‡æ¨¡æ‹Ÿé¢è¯•"
        ],
        "networking": [
            "ä¼˜åŒ–LinkedIn profileï¼ˆè‹±æ–‡ï¼‰",
            "å…³æ³¨ç›®æ ‡å…¬å¸å’Œè¡Œä¸šé¢†è¢–",
            "å‚åŠ çº¿ä¸Š/çº¿ä¸‹æ•°æ®å·¥ç¨‹meetup",
            "åœ¨æŠ€æœ¯ç¤¾åŒºï¼ˆReddit, Blindï¼‰æ´»è·ƒ",
            "è”ç³»åœ¨å¤–ä¼å·¥ä½œçš„æ ¡å‹æˆ–æœ‹å‹å†…æ¨"
        ],
        "target_companies": [
            "ä¼˜å…ˆçº§1ï¼ˆæ•°æ®å¯†é›†å‹ç§‘æŠ€å…¬å¸ï¼‰ï¼šMicrosoft, Amazon, Apple, SAP",
            "ä¼˜å…ˆçº§2ï¼ˆå’¨è¯¢å…¬å¸æ•°æ®å²—ï¼‰ï¼šAccenture, Deloitte, PwC, EY",
            "ä¼˜å…ˆçº§3ï¼ˆé‡‘èæœºæ„ï¼‰ï¼šHSBC, Citi, Goldman Sachs, JPMorgan",
            "ä¼˜å…ˆçº§4ï¼ˆå…¶ä»–å¤–ä¼ï¼‰ï¼šUnilever, Nike, BMWç­‰æœ‰æ•°æ®å›¢é˜Ÿçš„å…¬å¸"
        ]
    }

    return learning_plan


def save_analysis_report(jobs: List[Dict], skill_analysis: Dict, learning_plan: Dict, filename: str):
    """ä¿å­˜åˆ†ææŠ¥å‘Š"""
    report = {
        "generated_at": datetime.now().isoformat(),
        "location": "Shanghai, China",
        "focus": "Foreign Company Data Positions",
        "job_listings": jobs,
        "skill_analysis": skill_analysis,
        "learning_plan": learning_plan
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®Œæ•´åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")


def print_analysis_summary(skill_analysis: Dict, learning_plan: Dict):
    """æ‰“å°åˆ†ææ‘˜è¦"""
    print("\n" + "="*100)
    print("ğŸ“Š ä¸Šæµ·å¤–ä¼æ•°æ®å²—ä½åˆ†ææŠ¥å‘Š")
    print("="*100)
    print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"åˆ†æå²—ä½æ•°: {skill_analysis['total_jobs']}")

    print("\n" + "-"*100)
    print("ğŸ’° è–ªèµ„æ°´å¹³")
    print("-"*100)
    sal = skill_analysis['salary_analysis']
    print(f"å¹³å‡è–ªèµ„èŒƒå›´: {sal['average_low']} - {sal['average_high']}")
    print(f"æ€»ä½“èŒƒå›´: {sal['range']}")

    print("\n" + "-"*100)
    print("ğŸ”‘ Top 15 æ ¸å¿ƒæŠ€èƒ½è¦æ±‚")
    print("-"*100)
    for idx, (skill, count) in enumerate(list(skill_analysis['skill_frequency'].items())[:15], 1):
        percentage = (count / skill_analysis['total_jobs']) * 100
        bar = "â–ˆ" * int(percentage / 5)
        print(f"{idx:2d}. {skill:30s} {count:2d}æ¬¡ ({percentage:5.1f}%) {bar}")

    print("\n" + "-"*100)
    print("ğŸ’» æŠ€æœ¯æŠ€èƒ½åˆ†ç±»")
    print("-"*100)
    for skill, count in list(skill_analysis['technical_skills'].items())[:10]:
        print(f"  â€¢ {skill}: {count}æ¬¡")

    print("\n" + "-"*100)
    print("ğŸ› ï¸  å·¥å…·å’Œå¹³å°")
    print("-"*100)
    for skill, count in list(skill_analysis['tools_platforms'].items())[:10]:
        print(f"  â€¢ {skill}: {count}æ¬¡")

    print("\n" + "-"*100)
    print("ğŸ—£ï¸  è½¯æŠ€èƒ½å’Œå…¶ä»–è¦æ±‚")
    print("-"*100)
    for skill, count in skill_analysis['soft_skills'].items():
        print(f"  â€¢ {skill}: {count}æ¬¡")

    print("\n\n" + "="*100)
    print("ğŸ“š å­¦ä¹ è®¡åˆ’æ¦‚è§ˆ")
    print("="*100)
    print(f"æ—¶é—´çº¿: {learning_plan['overview']['timeline']}")
    print(f"ç›®æ ‡: {learning_plan['overview']['goal']}")
    print(f"\né‡ç‚¹é¢†åŸŸ:")
    for area in learning_plan['overview']['focus_areas']:
        print(f"  â€¢ {area}")

    print("\n" + "-"*100)
    print("ğŸ“… æœˆåº¦è®¡åˆ’")
    print("-"*100)
    for phase, details in learning_plan['monthly_plan'].items():
        print(f"\nã€{phase}ã€‘- {details['focus']}")
        print(f"æ¯å‘¨æŠ•å…¥: {details['weekly_hours']}")
        if details['skills']:
            print("å­¦ä¹ æŠ€èƒ½:")
            for skill_info in details['skills']:
                print(f"  â€¢ {skill_info['skill']} (å‡ºç°{skill_info['frequency']}æ¬¡) - {skill_info['details']['learning_time']}")
        if details.get('milestones'):
            print("é‡Œç¨‹ç¢‘:")
            for milestone in details['milestones']:
                print(f"  âœ“ {milestone}")

    print("\n" + "-"*100)
    print("ğŸ¯ æŠ€èƒ½è·¯çº¿å›¾")
    print("-"*100)
    for roadmap in learning_plan['skill_roadmap']:
        print(f"\n{roadmap['phase']}:")
        for goal in roadmap['goals']:
            print(f"  â€¢ {goal}")

    print("\n" + "-"*100)
    print("ğŸ† æ¨èè®¤è¯")
    print("-"*100)
    for cert in learning_plan['certifications'][:3]:  # æ˜¾ç¤ºå‰3ä¸ªæœ€é‡è¦çš„
        print(f"\n{cert['name']}")
        print(f"  éš¾åº¦: {cert['difficulty']} | å‡†å¤‡æ—¶é—´: {cert['prep_time']} | ä»·å€¼: {cert['value']}")

    print("\n" + "-"*100)
    print("ğŸŒ è‹±è¯­æå‡è®¡åˆ’")
    print("-"*100)
    print("æ¯æ—¥ç»ƒä¹ :")
    for routine in learning_plan['english_improvement']['daily_routine']:
        print(f"  â€¢ {routine}")
    print("\né˜¶æ®µç›®æ ‡:")
    for month, goal in learning_plan['english_improvement']['milestone_goals'].items():
        print(f"  â€¢ {month}: {goal}")

    print("\n" + "-"*100)
    print("ğŸ’¼ æ±‚èŒå‡†å¤‡")
    print("-"*100)
    print("\nä½œå“é›†é¡¹ç›®:")
    for project in learning_plan['job_preparation']['portfolio']:
        print(f"  â€¢ {project}")

    print("\nç›®æ ‡å…¬å¸:")
    for target in learning_plan['job_preparation']['target_companies']:
        print(f"  â€¢ {target}")

    print("\n\n" + "="*100)
    print("ğŸ“ˆ å…³é”®å»ºè®®")
    print("="*100)
    print("""
1. è‹±è¯­æ˜¯æœ€å¤§çš„å·®å¼‚åŒ–å› ç´ 
   å¤–ä¼å¯¹è‹±è¯­è¦æ±‚é«˜ï¼Œè¿™æ˜¯å›½å†…å…¬å¸è½¬å¤–ä¼æœ€å¤§çš„é—¨æ§›ä¹‹ä¸€ã€‚
   æ¯å¤©è‡³å°‘1å°æ—¶è‹±è¯­å­¦ä¹ ï¼Œé‡ç‚¹æ˜¯æŠ€æœ¯è‹±è¯­å’Œå£è¯­ã€‚

2. äº‘å¹³å°ç»éªŒæ˜¯å¿…å¤‡é¡¹
   90%çš„å¤–ä¼æ•°æ®å²—ä½è¦æ±‚äº‘å¹³å°ç»éªŒï¼ˆAWS/Azure/GCPï¼‰ã€‚
   ä¼˜å…ˆå­¦ä¹ AWSæˆ–Azureï¼Œé€šè¿‡è®¤è¯è¯æ˜èƒ½åŠ›ã€‚

3. ç°ä»£æ•°æ®æ ˆå·¥å…·åŠ åˆ†
   dbtã€Airflowã€Snowflakeç­‰ç°ä»£å·¥å…·åœ¨å¤–ä¼å¾ˆæµè¡Œã€‚
   è¿™æ˜¯ä¼ ç»Ÿæ•°ä»“èƒŒæ™¯è½¬å‹çš„é‡è¦çªç ´å£ã€‚

4. è½¯æŠ€èƒ½åŒæ ·é‡è¦
   å¤–ä¼é‡è§†æ²Ÿé€šã€åä½œã€æ–‡æ¡£èƒ½åŠ›ã€‚
   åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ³¨é‡ç”¨è‹±æ–‡å†™æ–‡æ¡£ã€åšpresentationã€‚

5. å†…æ¨æ˜¯æœ€æœ‰æ•ˆçš„é€”å¾„
   é€šè¿‡LinkedInè”ç³»ç›®æ ‡å…¬å¸å‘˜å·¥ï¼Œäº‰å–å†…æ¨æœºä¼šã€‚
   å‚åŠ è¡Œä¸šmeetupæ‰©å±•äººè„‰ã€‚

6. ä½œå“é›†å±•ç¤ºå®åŠ›
   GitHubä¸Šçš„é«˜è´¨é‡é¡¹ç›®æ¯”ç®€å†æ›´æœ‰è¯´æœåŠ›ã€‚
   ç¡®ä¿é¡¹ç›®æœ‰å®Œæ•´çš„è‹±æ–‡æ–‡æ¡£å’Œclearçš„æ¶æ„è¯´æ˜ã€‚

7. å¾ªåºæ¸è¿›ï¼Œä¸è¦æ€¥äºæ±‚æˆ
   3ä¸ªæœˆæ˜¯å¿«é€Ÿé€šé“ï¼Œå‹åŠ›å¤§ï¼›6ä¸ªæœˆæ›´ç¨³å¥ã€‚
   æ ¹æ®è‡ªå·±æƒ…å†µè°ƒæ•´å­¦ä¹ èŠ‚å¥ï¼Œè´¨é‡ä¼˜äºé€Ÿåº¦ã€‚
""")

    print("\nâœ¨ ç¥ä½ è½¬å‹æˆåŠŸï¼Start your journey today!")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆ†æä¸Šæµ·å¤–ä¼æ•°æ®å²—ä½...")
    print("ğŸ“ ç›®æ ‡: ä»å›½å†…äº’è”ç½‘ä¼ ç»Ÿæ•°ä»“å²—ä½ â†’ å¤–ä¼æ•°æ®å²—ä½")
    print("â±ï¸  æ—¶é—´çº¿: 3-6ä¸ªæœˆ\n")

    time.sleep(1)

    # è·å–æ¨¡æ‹Ÿçš„å²—ä½æ•°æ®
    print("ğŸ“¥ æ­£åœ¨è·å–å²—ä½ä¿¡æ¯...")
    jobs = simulate_job_listings()
    print(f"âœ“ è·å–åˆ° {len(jobs)} ä¸ªç›¸å…³å²—ä½")

    # åˆ†ææŠ€èƒ½è¦æ±‚
    print("\nğŸ” åˆ†æå²—ä½æŠ€èƒ½è¦æ±‚...")
    skill_analysis = analyze_skill_requirements(jobs)
    print("âœ“ æŠ€èƒ½åˆ†æå®Œæˆ")

    # å®šä¹‰å½“å‰æŠ€èƒ½ï¼ˆå›½å†…äº’è”ç½‘ä¼ ç»Ÿæ•°ä»“èƒŒæ™¯ï¼‰
    current_skills = {
        "SQL", "Hive", "Spark", "æ•°æ®å»ºæ¨¡", "ETL", "æ•°æ®ä»“åº“",
        "ç»´åº¦å»ºæ¨¡", "Python", "Shell", "Linux",
        "Hadoop", "æ•°æ®è´¨é‡", "ä¸­æ–‡æ²Ÿé€š"
    }

    # è¯†åˆ«æŠ€èƒ½å·®è·
    print("\nğŸ“Š è¯†åˆ«æŠ€èƒ½å·®è·...")
    skill_gaps = identify_skill_gaps(current_skills, skill_analysis['skill_frequency'])
    print(f"âœ“ å·²æœ‰æŠ€èƒ½è¦†ç›–ç‡: {skill_gaps['skill_coverage']:.1f}%")
    print(f"âœ“ éœ€è¦å­¦ä¹ çš„æ ¸å¿ƒæŠ€èƒ½: {len(skill_gaps['missing_skills'])} é¡¹")

    # ç”Ÿæˆå­¦ä¹ è®¡åˆ’ï¼ˆå¯é€‰æ‹©3ä¸ªæœˆæˆ–6ä¸ªæœˆï¼‰
    print("\nğŸ“š ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’...")
    timeline = 6  # å¯ä»¥æ”¹æˆ3
    learning_plan = generate_learning_plan(skill_gaps, timeline_months=timeline)
    print(f"âœ“ {timeline}ä¸ªæœˆå­¦ä¹ è®¡åˆ’å·²ç”Ÿæˆ")

    # ä¿å­˜å®Œæ•´æŠ¥å‘Š
    output_file = f"/Users/boom/Desktop/my_bussiness/Foreign company job opportunities/shanghai_data_jobs_analysis_{datetime.now().strftime('%Y%m%d')}.json"
    save_analysis_report(jobs, skill_analysis, learning_plan, output_file)

    # æ‰“å°æ‘˜è¦
    print_analysis_summary(skill_analysis, learning_plan)

    # ç”Ÿæˆé¢å¤–çš„æ–‡æœ¬ç‰ˆå­¦ä¹ è®¡åˆ’
    learning_plan_file = f"/Users/boom/Desktop/my_bussiness/Foreign company job opportunities/learning_plan_{timeline}months.txt"
    with open(learning_plan_file, 'w', encoding='utf-8') as f:
        f.write("="*100 + "\n")
        f.write(f"ä»ä¼ ç»Ÿæ•°ä»“åˆ°å¤–ä¼æ•°æ®å²—ä½ - {timeline}ä¸ªæœˆå­¦ä¹ è®¡åˆ’\n")
        f.write("="*100 + "\n\n")

        f.write("ç›®æ ‡:\n")
        f.write(f"{learning_plan['overview']['goal']}\n\n")

        f.write("æœˆåº¦è®¡åˆ’:\n")
        f.write("-"*100 + "\n")
        for phase, details in learning_plan['monthly_plan'].items():
            f.write(f"\n{phase}: {details['focus']}\n")
            f.write(f"æ¯å‘¨æŠ•å…¥: {details['weekly_hours']}\n\n")
            if details['skills']:
                f.write("å­¦ä¹ å†…å®¹:\n")
                for skill_info in details['skills']:
                    skill_name = skill_info['skill']
                    skill_details = skill_info['details']
                    f.write(f"\n  {skill_name} ({skill_details['learning_time']})\n")
                    f.write(f"  ä¼˜å…ˆçº§: {skill_details['priority']}\n")
                    f.write(f"  å­¦ä¹ èµ„æº:\n")
                    for resource in skill_details['resources']:
                        f.write(f"    - {resource}\n")
                    f.write(f"  å®è·µé¡¹ç›®:\n")
                    for project in skill_details['practice_projects']:
                        f.write(f"    - {project}\n")
            f.write("\n")
            if details.get('milestones'):
                f.write("  é‡Œç¨‹ç¢‘:\n")
                for milestone in details['milestones']:
                    f.write(f"    âœ“ {milestone}\n")
            f.write("\n")

        f.write("\n" + "="*100 + "\n")
        f.write("è‹±è¯­æå‡è®¡åˆ’\n")
        f.write("="*100 + "\n")
        f.write("\næ¯æ—¥ç»ƒä¹ :\n")
        for routine in learning_plan['english_improvement']['daily_routine']:
            f.write(f"  â€¢ {routine}\n")
        f.write("\næ¯å‘¨å®è·µ:\n")
        for practice in learning_plan['english_improvement']['weekly_practice']:
            f.write(f"  â€¢ {practice}\n")

        f.write("\n" + "="*100 + "\n")
        f.write("æ±‚èŒå‡†å¤‡æ¸…å•\n")
        f.write("="*100 + "\n")
        f.write("\nç®€å†å‡†å¤‡:\n")
        for item in learning_plan['job_preparation']['resume']:
            f.write(f"  â–¡ {item}\n")
        f.write("\nä½œå“é›†é¡¹ç›®:\n")
        for item in learning_plan['job_preparation']['portfolio']:
            f.write(f"  â–¡ {item}\n")
        f.write("\né¢è¯•å‡†å¤‡:\n")
        for item in learning_plan['job_preparation']['interview_prep']:
            f.write(f"  â–¡ {item}\n")

    print(f"\nâœ… å­¦ä¹ è®¡åˆ’æ–‡æœ¬ç‰ˆå·²ä¿å­˜åˆ°: {learning_plan_file}")
    print("\n" + "="*100)
    print("ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
    print("="*100)
    print("1. æŸ¥çœ‹å®Œæ•´çš„å­¦ä¹ è®¡åˆ’æ–‡ä»¶")
    print("2. æ ¹æ®è‡ªå·±çš„æ—¶é—´æƒ…å†µé€‰æ‹©3ä¸ªæœˆæˆ–6ä¸ªæœˆè®¡åˆ’")
    print("3. ç«‹å³å¼€å§‹ç¬¬ä¸€ä¸ªå­¦ä¹ ä»»åŠ¡")
    print("4. æ¯å‘¨å›é¡¾è¿›åº¦ï¼Œè°ƒï¿½ï¿½è®¡åˆ’")
    print("5. 3ä¸ªæœˆåå¼€å§‹æŠ•é€’ç®€å†ï¼ˆå¦‚æœé€‰æ‹©6ä¸ªæœˆè®¡åˆ’åˆ™æ˜¯5ä¸ªæœˆåï¼‰")
    print("\nğŸ’ª Success favors the prepared. Let's get started!")


if __name__ == "__main__":
    main()
