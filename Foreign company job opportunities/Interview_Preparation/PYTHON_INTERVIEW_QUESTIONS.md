# å¤–ä¼æ•°æ®å²—ä½ Python ç¬”è¯•é¢˜åº“ ğŸ

> åŸºäºçœŸå®å…¬å¸é¢è¯•é¢˜ | Microsoft, Amazon, Accenture, Goldman Sachsç­‰
> åŒ…å«22é“é«˜é¢‘é¢˜ç›® + è¯¦ç»†è§£ç­”

---

## ğŸ“‹ é¢˜åº“æ¦‚è§ˆ

| ç±»åˆ« | æ•°é‡ | é‡è¦æ€§ | å æ¯” |
|------|------|--------|------|
| æ•°æ®å¤„ç† (Pandas/Numpy) | 5é¢˜ | â­â­â­â­â­ | 40-50% |
| å­—ç¬¦ä¸²å’Œåˆ—è¡¨ | 5é¢˜ | â­â­â­â­â­ | 20-30% |
| å­—å…¸æ“ä½œ | 3é¢˜ | â­â­â­â­ | 15-20% |
| ç®—æ³•åŸºç¡€ | 5é¢˜ | â­â­â­â­ | 15-20% |
| å®é™…åœºæ™¯ | 4é¢˜ | â­â­â­â­ | 20-25% |

---

## ğŸ¯ å­¦ä¹ ç­–ç•¥

### æŒ‰å…¬å¸ç±»å‹å‡†å¤‡

**ç§‘æŠ€å…¬å¸ (Microsoft, Amazon, Apple)**:
- é‡ç‚¹ï¼šç®—æ³•åŸºç¡€ + Pandasæ•°æ®å¤„ç†
- éš¾åº¦ï¼šMediumä¸ºä¸»
- æ—¶é—´ï¼š45-60åˆ†é’Ÿå®æ—¶ç¼–ç 

**å’¨è¯¢å…¬å¸ (Accenture, Deloitte, PwC)**:
- é‡ç‚¹ï¼šæ•°æ®æ¸…æ´— + å®é™…åœºæ™¯
- éš¾åº¦ï¼šEasy-Medium
- æ—¶é—´ï¼š30-45åˆ†é’Ÿ

**é‡‘èæœºæ„ (HSBC, Goldman Sachs, Citi)**:
- é‡ç‚¹ï¼šæ•°æ®å¤„ç† + æ•°æ®éªŒè¯
- éš¾åº¦ï¼šMedium-Hard
- æ—¶é—´ï¼š60åˆ†é’Ÿ

### åˆ·é¢˜è®¡åˆ’

```
Week 1: æ•°æ®å¤„ç† (5é¢˜) + å­—ç¬¦ä¸²åˆ—è¡¨ (å‰3é¢˜)
Week 2: å­—ç¬¦ä¸²åˆ—è¡¨ (å2é¢˜) + å­—å…¸ (3é¢˜)
Week 3: ç®—æ³•åŸºç¡€ (5é¢˜)
Week 4: å®é™…åœºæ™¯ (4é¢˜) + æ€»å¤ä¹ 
```

---

## ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å¤„ç† (Pandas/Numpy) â­â­â­â­â­

### é¢˜ç›®1: æ•°æ®æ¸…æ´— - å¤„ç†ç¼ºå¤±å€¼å’Œé‡å¤å€¼
**éš¾åº¦**: â­â­ Medium
**æ¥æº**: Microsoft, Accenture
**è€ƒç‚¹**: pandasåŸºç¡€ã€æ•°æ®æ¸…æ´—

**é¢˜ç›®æè¿°**:
```
Given a DataFrame with missing values and duplicates, clean the data by:
1. Remove duplicate rows
2. Fill missing values in 'age' column with the mean
3. Drop rows where 'name' is missing
4. Return the cleaned DataFrame

Input DataFrame:
   name   age  city
0  Alice  25.0  NYC
1  Bob    NaN   LA
2  Alice  25.0  NYC
3  NaN    30.0  SF
4  David  35.0  NaN
```

**è§£ç­”**:
```python
import pandas as pd
import numpy as np

def clean_dataframe(df):
    # 1. Remove duplicates
    df = df.drop_duplicates()

    # 2. Fill missing age with mean
    mean_age = df['age'].mean()
    df['age'] = df['age'].fillna(mean_age)

    # 3. Drop rows where name is missing
    df = df.dropna(subset=['name'])

    # Reset index
    df = df.reset_index(drop=True)

    return df
```

**å…³é”®ç‚¹**:
- `drop_duplicates()` - å»é‡
- `fillna()` - å¡«å……ç¼ºå¤±å€¼
- `dropna(subset=[])` - åˆ é™¤ç‰¹å®šåˆ—çš„ç¼ºå¤±å€¼
- `reset_index()` - é‡ç½®ç´¢å¼•

**æ—¶é—´å¤æ‚åº¦**: O(n)
**ç©ºé—´å¤æ‚åº¦**: O(1) - in-placeæ“ä½œ

---

### é¢˜ç›®2: æ•°æ®åˆ†ç»„èšåˆ
**éš¾åº¦**: â­â­â­ Medium
**æ¥æº**: Amazon, Goldman Sachs
**è€ƒç‚¹**: groupbyã€èšåˆå‡½æ•°

**é¢˜ç›®æè¿°**:
```
Given a sales DataFrame, calculate:
1. Total revenue per product
2. Average price per product
3. Number of sales per product

Input:
   product  category     price  quantity
0  iPhone   Electronics  1000   2
1  MacBook  Electronics  2000   1
2  iPhone   Electronics  1000   3
3  AirPods  Electronics  200    5
```

**è§£ç­”**:
```python
def analyze_sales(df):
    # Calculate revenue
    df['revenue'] = df['price'] * df['quantity']

    # Group by product and aggregate
    summary = df.groupby('product').agg({
        'revenue': 'sum',
        'price': 'mean',
        'product': 'count'
    }).reset_index()

    # Rename columns
    summary.columns = ['product', 'total_revenue', 'avg_price', 'num_sales']

    return summary.sort_values('product')
```

**å…³é”®ç‚¹**:
- `groupby()` - åˆ†ç»„
- `agg()` - å¤šç§èšåˆå‡½æ•°
- `reset_index()` - å°†ç´¢å¼•å˜å›åˆ—

**å¸¸è§èšåˆå‡½æ•°**:
- `sum()`, `mean()`, `count()`, `min()`, `max()`
- `std()`, `var()`, `median()`

**æ—¶é—´å¤æ‚åº¦**: O(n log n) - å› ä¸ºæœ‰æ’åº
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®3: æ•°æ®é€è§† (Pivot)
**éš¾åº¦**: â­â­â­ Medium-Hard
**æ¥æº**: Accenture, Deloitte
**è€ƒç‚¹**: pivotã€æ•°æ®é‡å¡‘

**é¢˜ç›®æè¿°**:
```
Convert long-format to wide-format data.

Input:
   date        product  sales
0  2024-01-01  A        100
1  2024-01-01  B        200
2  2024-01-02  A        150
3  2024-01-02  B        250

Output:
date        A    B
2024-01-01  100  200
2024-01-02  150  250
```

**è§£ç­”**:
```python
def pivot_sales_data(df):
    pivoted = df.pivot(index='date', columns='product', values='sales')
    return pivoted

# å¦‚æœæœ‰é‡å¤å€¼ï¼Œä½¿ç”¨ pivot_table
def pivot_with_duplicates(df):
    pivoted = df.pivot_table(
        index='date',
        columns='product',
        values='sales',
        aggfunc='sum'  # æˆ– 'mean', 'count'
    )
    return pivoted
```

**å…³é”®ç‚¹**:
- `pivot()` - æ— é‡å¤æ—¶ä½¿ç”¨
- `pivot_table()` - æœ‰é‡å¤æ—¶ä½¿ç”¨ï¼Œéœ€è¦æŒ‡å®šèšåˆå‡½æ•°
- `index` - è¡Œç´¢å¼•
- `columns` - åˆ—å
- `values` - å¡«å……çš„å€¼

**æ—¶é—´å¤æ‚åº¦**: O(n)
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®4: æ—¶é—´åºåˆ—å¤„ç†
**éš¾åº¦**: â­â­â­ Medium
**æ¥æº**: HSBC, JPMorgan
**è€ƒç‚¹**: datetimeã€resampleã€æ»šåŠ¨çª—å£

**é¢˜ç›®æè¿°**:
```
Given timestamps and values, calculate:
1. Daily average
2. 7-day rolling average
3. Month-over-month growth rate
```

**è§£ç­”**:
```python
def analyze_timeseries(df):
    # Convert to datetime
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df = df.set_index('timestamp')

    # Daily average
    daily_avg = df.resample('D').mean()

    # 7-day rolling average
    daily_avg['rolling_7d'] = daily_avg['value'].rolling(window=7).mean()

    # Month-over-month growth
    monthly = df.resample('M').sum()
    monthly['mom_growth'] = monthly['value'].pct_change() * 100

    return daily_avg, monthly
```

**å¸¸ç”¨æ—¶é—´é¢‘ç‡**:
- `'D'` - å¤©
- `'W'` - å‘¨
- `'M'` - æœˆ
- `'Q'` - å­£åº¦
- `'Y'` - å¹´
- `'H'` - å°æ—¶

**å…³é”®å‡½æ•°**:
- `pd.to_datetime()` - è½¬æ¢ä¸ºæ—¥æœŸ
- `resample()` - é‡é‡‡æ ·
- `rolling()` - æ»šåŠ¨çª—å£
- `pct_change()` - ç™¾åˆ†æ¯”å˜åŒ–

---

### é¢˜ç›®5: æ•°æ®åˆå¹¶ (Join)
**éš¾åº¦**: â­â­â­ Medium
**æ¥æº**: Microsoft, Amazon
**è€ƒç‚¹**: mergeã€å¤šè¡¨å…³è”

**é¢˜ç›®æè¿°**:
```
Join three DataFrames (orders, customers, products)
to create a complete order report.
```

**è§£ç­”**:
```python
def merge_order_data(orders, customers, products):
    # Merge orders with customers
    result = orders.merge(customers, on='customer_id', how='left')

    # Merge with products
    result = result.merge(products, on='product_id', how='left')

    # Select columns
    result = result[['order_id', 'customer_name', 'product_name',
                     'quantity', 'price', 'order_date']]

    return result
```

**Joinç±»å‹**:
- `how='inner'` - å†…è¿æ¥ï¼ˆäº¤é›†ï¼‰
- `how='left'` - å·¦è¿æ¥ï¼ˆä¿ç•™å·¦è¡¨æ‰€æœ‰è®°å½•ï¼‰
- `how='right'` - å³è¿æ¥ï¼ˆä¿ç•™å³è¡¨æ‰€æœ‰è®°å½•ï¼‰
- `how='outer'` - å¤–è¿æ¥ï¼ˆå¹¶é›†ï¼‰

**æ—¶é—´å¤æ‚åº¦**: O(n + m)
**ç©ºé—´å¤æ‚åº¦**: O(n + m)

---

## ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šå­—ç¬¦ä¸²å’Œåˆ—è¡¨ â­â­â­â­â­

### é¢˜ç›®6: å­—ç¬¦ä¸²åˆ†æ
**éš¾åº¦**: â­ Easy
**æ¥æº**: åŸºç¡€é¢˜
**è€ƒç‚¹**: å­—ç¬¦ä¸²æ“ä½œã€åˆ—è¡¨æ¨å¯¼å¼

**é¢˜ç›®æè¿°**:
```
Given a sentence, return:
1. Reversed sentence (word order)
2. Unique words (case-insensitive)
3. Word count

Input: "Hello World Hello Python"
Output:
- Reversed: "Python Hello World Hello"
- Unique: ['hello', 'world', 'python']
- Count: 4
```

**è§£ç­”**:
```python
def analyze_sentence(sentence):
    words = sentence.split()

    # Reversed
    reversed_sentence = ' '.join(reversed(words))

    # Unique words
    unique_words = list(set(word.lower() for word in words))
    unique_words.sort()

    # Count
    word_count = len(words)

    return {
        'reversed': reversed_sentence,
        'unique': unique_words,
        'count': word_count
    }
```

**å¸¸ç”¨å­—ç¬¦ä¸²æ–¹æ³•**:
- `split()` - åˆ†å‰²
- `join()` - è¿æ¥
- `lower()`, `upper()` - å¤§å°å†™è½¬æ¢
- `strip()` - å»é™¤ç©ºç™½
- `replace()` - æ›¿æ¢

**æ—¶é—´å¤æ‚åº¦**: O(n)
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®7: Top K é¢‘ç‡å…ƒç´ 
**éš¾åº¦**: â­â­ Easy-Medium
**æ¥æº**: Amazon, Microsoft
**è€ƒç‚¹**: Counterã€å“ˆå¸Œè¡¨

**é¢˜ç›®æè¿°**:
```
Find the top K most frequent elements.

Input: [1, 1, 1, 2, 2, 3], k=2
Output: [1, 2]
```

**è§£ç­”**:
```python
from collections import Counter

def top_k_frequent(nums, k):
    counter = Counter(nums)
    top_k = [item for item, count in counter.most_common(k)]
    return top_k

# æ–¹æ³•2: ä½¿ç”¨heapï¼ˆå¤§æ•°æ®é›†æ›´é«˜æ•ˆï¼‰
import heapq

def top_k_frequent_heap(nums, k):
    counter = Counter(nums)
    return heapq.nlargest(k, counter.keys(), key=counter.get)
```

**å…³é”®ç‚¹**:
- `Counter()` - ç»Ÿè®¡é¢‘ç‡
- `most_common(k)` - è·å–å‰kä¸ª
- `heapq.nlargest()` - å †å®ç°

**æ—¶é—´å¤æ‚åº¦**:
- Counteræ–¹æ³•: O(n log n)
- Heapæ–¹æ³•: O(n log k) - æ›´ä¼˜

**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®8: åˆå¹¶åŒºé—´
**éš¾åº¦**: â­â­â­ Medium
**æ¥æº**: Amazon (LeetCode 56)
**è€ƒç‚¹**: æ’åºã€åŒºé—´å¤„ç†

**é¢˜ç›®æè¿°**:
```
Merge overlapping intervals.

Input: [[1,3],[2,6],[8,10],[15,18]]
Output: [[1,6],[8,10],[15,18]]
```

**è§£ç­”**:
```python
def merge_intervals(intervals):
    if not intervals:
        return []

    # Sort by start time
    intervals.sort(key=lambda x: x[0])

    merged = [intervals[0]]

    for current in intervals[1:]:
        last = merged[-1]

        if current[0] <= last[1]:  # Overlapping
            last[1] = max(last[1], current[1])
        else:
            merged.append(current)

    return merged
```

**å…³é”®æ€è·¯**:
1. å…ˆæŒ‰èµ·å§‹ä½ç½®æ’åº
2. éå†ï¼Œæ£€æŸ¥å½“å‰åŒºé—´æ˜¯å¦ä¸ä¸Šä¸€ä¸ªé‡å 
3. é‡å åˆ™åˆå¹¶ï¼Œå¦åˆ™æ·»åŠ æ–°åŒºé—´

**æ—¶é—´å¤æ‚åº¦**: O(n log n) - æ’åºä¸»å¯¼
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®9: å»é‡ä¿æŒé¡ºåº
**éš¾åº¦**: â­â­ Easy-Medium
**æ¥æº**: Accenture, Deloitte
**è€ƒç‚¹**: é›†åˆã€é¡ºåºä¿æŒ

**é¢˜ç›®æè¿°**:
```
Remove duplicates while preserving order.

Input: [1, 2, 3, 2, 4, 3, 5]
Output: [1, 2, 3, 4, 5]
```

**è§£ç­”**:
```python
# æ–¹æ³•1: ä½¿ç”¨setå’Œlist
def remove_duplicates(lst):
    seen = set()
    result = []

    for item in lst:
        if item not in seen:
            seen.add(item)
            result.append(item)

    return result

# æ–¹æ³•2: ä½¿ç”¨dict (Python 3.7+ä¿åº)
def remove_duplicates_dict(lst):
    return list(dict.fromkeys(lst))
```

**æ—¶é—´å¤æ‚åº¦**: O(n)
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®10: FlattenåµŒå¥—åˆ—è¡¨
**éš¾åº¦**: â­â­ Medium
**æ¥æº**: Microsoft, Goldman Sachs
**è€ƒç‚¹**: é€’å½’ã€åˆ—è¡¨æ“ä½œ

**é¢˜ç›®æè¿°**:
```
Flatten a nested list of arbitrary depth.

Input: [1, [2, 3, [4, 5]], 6, [7]]
Output: [1, 2, 3, 4, 5, 6, 7]
```

**è§£ç­”**:
```python
def flatten_list(nested_list):
    result = []

    for item in nested_list:
        if isinstance(item, list):
            result.extend(flatten_list(item))  # Recursive
        else:
            result.append(item)

    return result

# æ–¹æ³•2: ä½¿ç”¨generatorï¼ˆå†…å­˜é«˜æ•ˆï¼‰
def flatten_generator(nested_list):
    for item in nested_list:
        if isinstance(item, list):
            yield from flatten_generator(item)
        else:
            yield item

# ä½¿ç”¨: list(flatten_generator(nested_list))
```

**æ—¶é—´å¤æ‚åº¦**: O(n) - næ˜¯æ€»å…ƒç´ æ•°
**ç©ºé—´å¤æ‚åº¦**: O(d) - dæ˜¯æœ€å¤§æ·±åº¦ï¼ˆé€’å½’æ ˆï¼‰

---

## ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šå­—å…¸æ“ä½œ â­â­â­â­

### é¢˜ç›®11: å­—å…¸åè½¬
**éš¾åº¦**: â­â­ Easy-Medium
**æ¥æº**: Accenture
**è€ƒç‚¹**: å­—å…¸æ“ä½œ

**é¢˜ç›®æè¿°**:
```
Reverse dictionary keys and values.
If multiple keys have same value, collect them in a list.

Input: {'a': 1, 'b': 2, 'c': 1, 'd': 3}
Output: {1: ['a', 'c'], 2: ['b'], 3: ['d']}
```

**è§£ç­”**:
```python
def reverse_dictionary(d):
    reversed_dict = {}

    for key, value in d.items():
        if value not in reversed_dict:
            reversed_dict[value] = []
        reversed_dict[value].append(key)

    return reversed_dict

# æ–¹æ³•2: ä½¿ç”¨defaultdict
from collections import defaultdict

def reverse_dict_defaultdict(d):
    reversed_dict = defaultdict(list)
    for key, value in d.items():
        reversed_dict[value].append(key)
    return dict(reversed_dict)
```

**æ—¶é—´å¤æ‚åº¦**: O(n)
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®12: åˆå¹¶å¤šä¸ªå­—å…¸
**éš¾åº¦**: â­â­ Medium
**æ¥æº**: Microsoft, Amazon
**è€ƒç‚¹**: å­—å…¸åˆå¹¶ã€å€¼ç´¯åŠ 

**é¢˜ç›®æè¿°**:
```
Merge dictionaries and sum values for duplicate keys.

Input: [{'a': 1, 'b': 2}, {'b': 3, 'c': 4}, {'a': 5}]
Output: {'a': 6, 'b': 5, 'c': 4}
```

**è§£ç­”**:
```python
def merge_dictionaries(dicts):
    result = {}

    for d in dicts:
        for key, value in d.items():
            result[key] = result.get(key, 0) + value

    return result

# æ–¹æ³•2: ä½¿ç”¨Counter
from collections import Counter

def merge_dicts_counter(dicts):
    result = Counter()
    for d in dicts:
        result.update(d)
    return dict(result)
```

**æ—¶é—´å¤æ‚åº¦**: O(nÃ—m) - nä¸ªå­—å…¸ï¼Œå¹³å‡mä¸ªé”®
**ç©ºé—´å¤æ‚åº¦**: O(k) - kä¸ªå”¯ä¸€é”®

---

### é¢˜ç›®13: å­—å…¸æ’åº
**éš¾åº¦**: â­â­ Easy-Medium
**æ¥æº**: é€šç”¨é¢˜
**è€ƒç‚¹**: æ’åº

**é¢˜ç›®æè¿°**:
```
Sort dictionary by values (descending), then keys (ascending).

Input: {'apple': 3, 'banana': 1, 'cherry': 3, 'date': 2}
Output: [('apple', 3), ('cherry', 3), ('date', 2), ('banana', 1)]
```

**è§£ç­”**:
```python
def sort_dictionary(d):
    # Sort by value (desc), then key (asc)
    sorted_items = sorted(d.items(), key=lambda x: (-x[1], x[0]))
    return sorted_items
```

**æ—¶é—´å¤æ‚åº¦**: O(n log n)
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

## ğŸ“š ç¬¬å››éƒ¨åˆ†ï¼šç®—æ³•åŸºç¡€ â­â­â­â­

### é¢˜ç›®14: Two Sum
**éš¾åº¦**: â­ Easy
**æ¥æº**: Amazon, Microsoft (LeetCode 1)
**è€ƒç‚¹**: å“ˆå¸Œè¡¨

**é¢˜ç›®æè¿°**:
```
Find two numbers that add up to target.

Input: nums = [2, 7, 11, 15], target = 9
Output: [0, 1]
```

**è§£ç­”**:
```python
def two_sum(nums, target):
    seen = {}  # value -> index

    for i, num in enumerate(nums):
        complement = target - num
        if complement in seen:
            return [seen[complement], i]
        seen[num] = i

    return []
```

**æ—¶é—´å¤æ‚åº¦**: O(n)
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®15: æœ‰æ•ˆæ‹¬å·
**éš¾åº¦**: â­ Easy
**æ¥æº**: Microsoft, Goldman Sachs (LeetCode 20)
**è€ƒç‚¹**: æ ˆ

**é¢˜ç›®æè¿°**:
```
Check if parentheses are valid.

Input: "([)]"
Output: False

Input: "()[]{}"
Output: True
```

**è§£ç­”**:
```python
def is_valid_parentheses(s):
    stack = []
    mapping = {')': '(', '}': '{', ']': '['}

    for char in s:
        if char in mapping:  # Closing bracket
            if not stack or stack[-1] != mapping[char]:
                return False
            stack.pop()
        else:  # Opening bracket
            stack.append(char)

    return len(stack) == 0
```

**å…³é”®æ€è·¯**:
- é‡åˆ°å·¦æ‹¬å·ï¼Œå…¥æ ˆ
- é‡åˆ°å³æ‹¬å·ï¼Œæ£€æŸ¥æ ˆé¡¶æ˜¯å¦åŒ¹é…
- æœ€åæ ˆå¿…é¡»ä¸ºç©º

**æ—¶é—´å¤æ‚åº¦**: O(n)
**ç©ºé—´å¤æ‚åº¦**: O(n)

---

### é¢˜ç›®16: æœ€å¤§å­æ•°ç»„å’Œ (Kadane's Algorithm)
**éš¾åº¦**: â­â­ Easy-Medium
**æ¥æº**: Microsoft, Amazon (LeetCode 53)
**è€ƒç‚¹**: åŠ¨æ€è§„åˆ’

**é¢˜ç›®æè¿°**:
```
Find the maximum sum of a contiguous subarray.

Input: [-2,1,-3,4,-1,2,1,-5,4]
Output: 6 (subarray [4,-1,2,1])
```

**è§£ç­”**:
```python
def max_subarray(nums):
    if not nums:
        return 0

    max_sum = current_sum = nums[0]

    for num in nums[1:]:
        # Either extend or start new
        current_sum = max(num, current_sum + num)
        max_sum = max(max_sum, current_sum)

    return max_sum
```

**å…³é”®æ€è·¯**:
- æ¯ä¸ªä½ç½®é€‰æ‹©ï¼šè¦ä¹ˆåŠ å…¥å½“å‰å­æ•°ç»„ï¼Œè¦ä¹ˆé‡æ–°å¼€å§‹
- `current_sum = max(num, current_sum + num)`

**æ—¶é—´å¤æ‚åº¦**: O(n)
**ç©ºé—´å¤æ‚åº¦**: O(1)

---

### é¢˜ç›®17: äºŒåˆ†æŸ¥æ‰¾
**éš¾åº¦**: â­â­ Easy
**æ¥æº**: åŸºç¡€ç®—æ³•
**è€ƒç‚¹**: äºŒåˆ†æœç´¢

**é¢˜ç›®æè¿°**:
```
Find target in sorted array using binary search.

Input: nums = [1, 3, 5, 7, 9, 11], target = 7
Output: 3
```

**è§£ç­”**:
```python
def binary_search(nums, target):
    left, right = 0, len(nums) - 1

    while left <= right:
        mid = (left + right) // 2

        if nums[mid] == target:
            return mid
        elif nums[mid] < target:
            left = mid + 1
        else:
            right = mid - 1

    return -1
```

**æ—¶é—´å¤æ‚åº¦**: O(log n)
**ç©ºé—´å¤æ‚åº¦**: O(1)

---

### é¢˜ç›®18: æ–æ³¢é‚£å¥‘æ•°åˆ—
**éš¾åº¦**: â­ Easy
**æ¥æº**: åŸºç¡€é¢˜
**è€ƒç‚¹**: é€’å½’ã€DP

**é¢˜ç›®æè¿°**:
```
Calculate nth Fibonacci number.

Input: n = 6
Output: 8
```

**è§£ç­”**:
```python
# æ–¹æ³•1: è¿­ä»£ï¼ˆæ¨èï¼‰
def fibonacci(n):
    if n <= 1:
        return n

    prev, curr = 0, 1
    for _ in range(2, n + 1):
        prev, curr = curr, prev + curr

    return curr

# æ–¹æ³•2: é€’å½’ + è®°å¿†åŒ–
def fibonacci_memo(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n

    memo[n] = fibonacci_memo(n-1, memo) + fibonacci_memo(n-2, memo)
    return memo[n]
```

**æ—¶é—´å¤æ‚åº¦**:
- è¿­ä»£: O(n)
- é€’å½’+memo: O(n)
- çº¯é€’å½’: O(2^n) - ä¸æ¨è

**ç©ºé—´å¤æ‚åº¦**:
- è¿­ä»£: O(1)
- é€’å½’+memo: O(n)

---

## ğŸ“š ç¬¬äº”éƒ¨åˆ†ï¼šå®é™…åœºæ™¯ â­â­â­â­

### é¢˜ç›®19: è§£æJSON
**éš¾åº¦**: â­â­ Medium
**æ¥æº**: Microsoft, Amazon
**è€ƒç‚¹**: JSONå¤„ç†ã€æ•°æ®æ‰å¹³åŒ–

**é¢˜ç›®æè¿°**:
```
Parse JSON strings and flatten nested structure into DataFrame.

Input:
[
  '{"id": 1, "name": "Alice", "address": {"city": "NYC", "zip": "10001"}}',
  '{"id": 2, "name": "Bob", "address": {"city": "LA", "zip": "90001"}}'
]

Output DataFrame:
   id  name  city  zip
0  1   Alice NYC   10001
1  2   Bob   LA    90001
```

**è§£ç­”**:
```python
import json
import pandas as pd

def parse_json_to_dataframe(json_strings):
    data = []

    for json_str in json_strings:
        obj = json.loads(json_str)

        # Flatten
        flat_obj = {
            'id': obj['id'],
            'name': obj['name'],
            'city': obj['address']['city'],
            'zip': obj['address']['zip']
        }
        data.append(flat_obj)

    return pd.DataFrame(data)
```

**å…³é”®ç‚¹**:
- `json.loads()` - è§£æJSONå­—ç¬¦ä¸²
- æ‰‹åŠ¨æ‰å¹³åŒ–åµŒå¥—ç»“æ„
- åˆ›å»ºDataFrame

**æ—¶é—´å¤æ‚åº¦**: O(nÃ—m) - næ¡è®°å½•ï¼Œmä¸ªå­—æ®µ
**ç©ºé—´å¤æ‚åº¦**: O(nÃ—m)

---

### é¢˜ç›®20: æ•°æ®éªŒè¯
**éš¾åº¦**: â­â­â­ Medium
**æ¥æº**: Consulting firms, Financial companies
**è€ƒç‚¹**: æ•°æ®è´¨é‡æ£€æŸ¥

**é¢˜ç›®æè¿°**:
```
Validate DataFrame and return error report:
1. Check null values in required columns
2. Validate email format
3. Validate age range (0-120)
```

**è§£ç­”**:
```python
import re

def validate_data(df):
    errors = {
        'null_values': {},
        'invalid_email': [],
        'invalid_age': []
    }

    # Check nulls
    required_cols = ['name', 'email']
    for col in required_cols:
        null_indices = df[df[col].isnull()].index.tolist()
        if null_indices:
            errors['null_values'][col] = null_indices

    # Validate email
    email_pattern = r'^[\w\.-]+@[\w\.-]+\.\w+$'
    for idx, email in df['email'].items():
        if pd.notna(email) and not re.match(email_pattern, str(email)):
            errors['invalid_email'].append(idx)
        elif pd.isna(email):
            errors['invalid_email'].append(idx)

    # Validate age
    for idx, age in df['age'].items():
        if pd.notna(age) and (age < 0 or age > 120):
            errors['invalid_age'].append(idx)

    return errors
```

**å…³é”®ç‚¹**:
- æ­£åˆ™è¡¨è¾¾å¼éªŒè¯æ ¼å¼
- èŒƒå›´æ£€æŸ¥
- è¿”å›è¯¦ç»†é”™è¯¯æŠ¥å‘Š

---

### é¢˜ç›®21: SQLç»“æœè½¬æ¢
**éš¾åº¦**: â­â­ Medium
**æ¥æº**: æ•°æ®å·¥ç¨‹å²—ä½
**è€ƒç‚¹**: æ•°æ®è½¬æ¢

**é¢˜ç›®æè¿°**:
```
Convert SQL query result (list of tuples) to:
1. List of dictionaries
2. Dictionary grouped by a key

Input: [('Alice', 'Sales', 5000), ('Bob', 'IT', 6000)]
Columns: ['name', 'department', 'salary']
```

**è§£ç­”**:
```python
def convert_sql_result(rows, columns):
    # 1. List of dicts
    list_of_dicts = [dict(zip(columns, row)) for row in rows]

    # 2. Grouped by department
    grouped = {}
    for row in rows:
        dept = row[1]  # department
        if dept not in grouped:
            grouped[dept] = []
        grouped[dept].append({
            'name': row[0],
            'salary': row[2]
        })

    return list_of_dicts, grouped
```

**å…³é”®æŠ€å·§**:
- `zip()` é…å¯¹åˆ—åå’Œå€¼
- `dict()` åˆ›å»ºå­—å…¸
- åˆ—è¡¨æ¨å¯¼å¼

---

### é¢˜ç›®22: æ‰¹é‡æ–‡ä»¶å¤„ç†
**éš¾åº¦**: â­â­â­ Medium-Hard
**æ¥æº**: Real ETL scenarios
**è€ƒç‚¹**: æ–‡ä»¶å¤„ç†ã€é”™è¯¯å¤„ç†

**é¢˜ç›®æè¿°**:
```
Process multiple CSV files in a directory:
1. Read all CSV files
2. Validate and clean data
3. Combine into one DataFrame
4. Handle errors gracefully
```

**è§£ç­”**:
```python
import os
import pandas as pd
from pathlib import Path

def process_csv_files(directory):
    all_data = []
    errors = []

    # Get all CSV files
    csv_files = Path(directory).glob('*.csv')

    for file_path in csv_files:
        try:
            # Read file
            df = pd.read_csv(file_path)

            # Validate
            required_cols = ['id', 'date', 'value']
            if not all(col in df.columns for col in required_cols):
                errors.append(f"{file_path}: Missing required columns")
                continue

            # Clean
            df = df.dropna(subset=required_cols)
            df['file_source'] = file_path.name

            all_data.append(df)

        except Exception as e:
            errors.append(f"{file_path}: {str(e)}")
            continue

    # Combine all
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        return combined_df, errors
    else:
        return None, errors
```

**å…³é”®ç‚¹**:
- `Path.glob()` - åŒ¹é…æ–‡ä»¶
- `try-except` - é”™è¯¯å¤„ç†
- `pd.concat()` - åˆå¹¶å¤šä¸ªDataFrame

---

## ğŸ¯ é¢è¯•æŠ€å·§

### 1. è§£é¢˜æ­¥éª¤

```
1. æ¾„æ¸…éœ€æ±‚ (1-2åˆ†é’Ÿ)
   - è¾“å…¥æ ¼å¼å’ŒèŒƒå›´
   - è¾“å‡ºè¦æ±‚
   - Edge cases
   - æ€§èƒ½è¦æ±‚

2. è¯´æ˜æ€è·¯ (2-3åˆ†é’Ÿ)
   - ç”¨è‹±æ–‡è®²è§£approach
   - åˆ†ææ—¶é—´ç©ºé—´å¤æ‚åº¦
   - è®¨è®ºtrade-offs

3. ç¼–å†™ä»£ç  (10-15åˆ†é’Ÿ)
   - è¾¹å†™è¾¹è®²è§£
   - å†™clean code
   - ä½¿ç”¨æœ‰æ„ä¹‰çš„å˜é‡å

4. æµ‹è¯•éªŒè¯ (3-5åˆ†é’Ÿ)
   - èµ°ä¸€éæ­£å¸¸case
   - æ£€æŸ¥edge cases
   - è€ƒè™‘ä¼˜åŒ–
```

### 2. è‹±æ–‡è¡¨è¾¾

**å¼€åœº**:
- "Let me make sure I understand the problem..."
- "Can I clarify a few things?"
- "Here's my approach..."

**åˆ†æ**:
- "The time complexity would be O(n)"
- "We can optimize this by using a hash table"
- "The trade-off here is..."

**ç¼–ç ä¸­**:
- "First, I'll handle the edge case..."
- "Let me add a helper function for..."
- "This variable represents..."

**æ£€æŸ¥**:
- "Let me walk through an example"
- "I should check for null values here"
- "One optimization could be..."

### 3. å¸¸è§é”™è¯¯

âŒ **ä¸è¦åš**:
- ç›´æ¥å¼€å§‹å†™ä»£ç 
- æ²‰é»˜ä¸è¯­
- å†™å®Œä¸æµ‹è¯•
- å¿½ç•¥edge cases
- å˜é‡åå¤ªéšæ„

âœ… **è¦åš**:
- Think out loud
- å†™æ¸…æ™°çš„ä»£ç 
- è€ƒè™‘è¾¹ç•Œæƒ…å†µ
- è®¨è®ºä¼˜åŒ–æ–¹æ¡ˆ
- æµ‹è¯•ä½ çš„ä»£ç 

---

## ğŸ“ ç»ƒä¹ è®¡åˆ’

### Week 1: æ•°æ®å¤„ç†åŸºç¡€
**ç›®æ ‡**: æŒæ¡Pandasæ ¸å¿ƒæ“ä½œ

| æ—¥æœŸ | é¢˜ç›® | æ—¶é—´ |
|------|------|------|
| Day 1 | é¢˜ç›®1 - æ•°æ®æ¸…æ´— | 30åˆ†é’Ÿ |
| Day 2 | é¢˜ç›®2 - åˆ†ç»„èšåˆ | 30åˆ†é’Ÿ |
| Day 3 | é¢˜ç›®3 - æ•°æ®é€è§† | 45åˆ†é’Ÿ |
| Day 4 | é¢˜ç›®4 - æ—¶é—´åºåˆ— | 45åˆ†é’Ÿ |
| Day 5 | é¢˜ç›®5 - æ•°æ®åˆå¹¶ | 30åˆ†é’Ÿ |
| Day 6-7 | å¤ä¹ +LeetCode Pandasé¢˜ | 2å°æ—¶ |

### Week 2: å­—ç¬¦ä¸²å’Œå­—å…¸
**ç›®æ ‡**: ç†Ÿç»ƒåŸºç¡€æ•°æ®ç»“æ„

| æ—¥æœŸ | é¢˜ç›® | æ—¶é—´ |
|------|------|------|
| Day 1-2 | é¢˜ç›®6-10 (å­—ç¬¦ä¸²åˆ—è¡¨) | å„30åˆ†é’Ÿ |
| Day 3-4 | é¢˜ç›®11-13 (å­—å…¸) | å„30åˆ†é’Ÿ |
| Day 5 | ç»¼åˆç»ƒä¹  | 1å°æ—¶ |
| Day 6-7 | å¤ä¹ +é¢å¤–é¢˜ç›® | 2å°æ—¶ |

### Week 3: ç®—æ³•åŸºç¡€
**ç›®æ ‡**: æŒæ¡åŸºç¡€ç®—æ³•

| æ—¥æœŸ | é¢˜ç›® | æ—¶é—´ |
|------|------|------|
| Day 1-5 | é¢˜ç›®14-18 (ç®—æ³•) | å„45åˆ†é’Ÿ |
| Day 6-7 | LeetCode Easyé¢˜10é“ | 3å°æ—¶ |

### Week 4: å®æˆ˜æ¨¡æ‹Ÿ
**ç›®æ ‡**: ç»¼åˆåº”ç”¨

| æ—¥æœŸ | ä»»åŠ¡ | æ—¶é—´ |
|------|------|------|
| Day 1-3 | é¢˜ç›®19-22 (å®é™…åœºæ™¯) | å„1å°æ—¶ |
| Day 4-5 | æ¨¡æ‹Ÿå®Œæ•´é¢è¯• | å„1.5å°æ—¶ |
| Day 6-7 | å¤ä¹ è–„å¼±ç¯èŠ‚ | 3å°æ—¶ |

---

## ğŸ’» æ¨èèµ„æº

### åœ¨çº¿åˆ·é¢˜å¹³å°
- **LeetCode** - Pandasé¢˜ç›® + ç®—æ³•é¢˜
- **HackerRank** - Python + SQL
- **StrataScratch** - çœŸå®å…¬å¸æ•°æ®é¢˜
- **DataLemur** - æ•°æ®åˆ†æå¸ˆé¢˜åº“

### å­¦ä¹ èµ„æº
- **ä¹¦ç±**:
  - ã€ŠPython for Data Analysisã€‹ - Wes McKinney
  - ã€ŠPandas Cookbookã€‹

- **è¯¾ç¨‹**:
  - DataCamp - Pandasè¯¾ç¨‹
  - Coursera - Python for Data Science

### Mock Interview
- **Pramp** - å…è´¹peer interview
- **interviewing.io** - æŠ€æœ¯é¢è¯•ç»ƒä¹ 

---

## ğŸš€ æœ€åçš„è¯

**é‡ç‚¹å‡†å¤‡é¡ºåº**:
1. Pandasæ•°æ®å¤„ç† (40-50%é¢è¯•å†…å®¹)
2. åŸºç¡€ç®—æ³• (Two Sum, å­—ç¬¦ä¸²ç­‰)
3. å®é™…åœºæ™¯é¢˜ (ETL, æ•°æ®æ¸…æ´—)
4. ç»ƒä¹ è‹±æ–‡è¡¨è¾¾

**æ¯å¤©ç»ƒä¹ **:
- å·¥ä½œæ—¥: 1-2é“é¢˜ (1-1.5å°æ—¶)
- å‘¨æœ«: 3-5é“é¢˜ (2-3å°æ—¶)
- 4å‘¨å¯ä»¥å‡†å¤‡å……åˆ†

**é¢è¯•å‰**:
- å¿«é€Ÿè¿‡ä¸€éæ‰€æœ‰é¢˜ç›®
- ç»ƒä¹ è‹±æ–‡è®²è§£æ€è·¯
- å‡†å¤‡3-5ä¸ªå¸¸è§é¢˜çš„æ¨¡æ¿

**Good luck with your Python interviews! ğŸ’ªğŸ**

---

## ğŸ“ ç›¸å…³æ–‡ä»¶

- [python_interview_questions.py](./python_interview_questions.py) - å®Œæ•´ä»£ç 
- [INTERVIEW_QUICK_GUIDE.md](./INTERVIEW_QUICK_GUIDE.md) - é¢è¯•æ€»æŒ‡å—
- [interview_prep_complete.json](./interview_prep_complete.json) - å®Œæ•´å‡†å¤‡ææ–™
